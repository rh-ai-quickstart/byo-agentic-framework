{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# LangChain Agents and Llama Stack with MCP Tools\n\nThis notebook demonstrates **LangChain 1.0** integration with **Llama Stack** responses API using proper **MCP (Model Context Protocol) adapters** for client-side tool execution.\n\n## Configuration\n\nThis notebook uses environment variables from `.env` file in the project root.\nCreate your own `.env` file based on `.env.example`.\n\n## Architecture\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚         LangChain 1.0 Agent                 â”‚\nâ”‚         (create_agent)                      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â”‚                    â”‚\n         â”‚ Model Calls        â”‚ Tool Execution\n         â–¼                    â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Llama Stack    â”‚   â”‚  MCP Client          â”‚\nâ”‚  (OpenAI API)   â”‚   â”‚  (langchain-mcp)     â”‚\nâ”‚  - vLLM         â”‚   â”‚  - SSE Transport     â”‚\nâ”‚  - Llama 3.2    â”‚   â”‚  - Tool Adapters     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                             â”‚\n                             â–¼\n                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                      â”‚ MCP Server   â”‚\n                      â”‚ (Weather)    â”‚\n                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Packages\n",
    "\n",
    "Install LangChain 1.0 and MCP adapters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.12 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -q \"langchain>=1.0\" \"langchain-openai>=0.3.32\" \"langchain-core>=0.3.75\" \"langchain-mcp-adapters>=0.1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Core imports\nimport os\nimport sys\nimport asyncio\nfrom pathlib import Path\nfrom pprint import pprint\nfrom dotenv import load_dotenv\n\n# LangChain 1.0 imports\nfrom langchain.agents import create_agent\nfrom langchain_openai import ChatOpenAI\n\n# MCP Adapters for client-side tool execution\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\n\n# Load environment variables from project root\nproject_root = Path.cwd().parent.parent if 'assets/notebooks' in str(Path.cwd()) else Path.cwd()\nenv_path = project_root / '.env'\nload_dotenv(env_path)\n\nprint(f\"ğŸ“ Loading environment from: {env_path}\")\nprint(f\"âœ… .env file {'found' if env_path.exists() else 'NOT FOUND'}\")\nprint(\"âœ… All dependencies imported successfully\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Llama Stack Connection\n",
    "\n",
    "Connect to Llama Stack's OpenAI-compatible endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === Llama Stack Configuration ===\n# Load from environment variables\nLLAMA_STACK_OPENAI_ENDPOINT = os.getenv(\"LLAMA_STACK_OPENAI_ENDPOINT\")\nINFERENCE_MODEL = os.getenv(\"INFERENCE_MODEL\")\nAPI_KEY = os.getenv(\"API_KEY\", \"fake\")\n\nprint(\"ğŸŒ Llama Stack Configuration:\")\nprint(f\"   Endpoint using Responses API: {LLAMA_STACK_OPENAI_ENDPOINT}\")\nprint(f\"   Model: {INFERENCE_MODEL}\")\n\n# Create ChatOpenAI client pointing to Llama Stack\nllm = ChatOpenAI(\n    model=INFERENCE_MODEL,\n    api_key=API_KEY,\n    base_url=LLAMA_STACK_OPENAI_ENDPOINT,\n    temperature=0.0,\n)\n\nprint(\"\\nğŸ§ª Testing connectivity...\")\ntry:\n    response = llm.invoke(\"Say 'Connection successful' if you can read this.\")\n    print(f\"ğŸ“¥ LLM Response: {response.content}\")\n    print(f\"ğŸ“¥ LLM Response metadata: {response.response_metadata}\")\n    print(\"âœ… Llama Stack connection successful!\")\nexcept Exception as e:\n    print(f\"âŒ Connection failed: {e}\")\n    sys.exit(1)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up MCP Client and Tools\n",
    "\n",
    "Configure the MCP client to connect to the weather service using SSE transport:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === MCP Client Configuration ===\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\n\n# Load MCP server URL from environment\nMCP_WEATHER_SERVER_URL = os.getenv(\"MCP_WEATHER_SERVER_URL\")\n\nprint(\"ğŸ› ï¸  Configuring MCP client...\\n\")\nprint(f\"ğŸŒ¦ï¸  MCP Server URL: {MCP_WEATHER_SERVER_URL}\")\n\n# Step 1: Create the MCP client\nclient = MultiServerMCPClient({\n    \"weather\": {\n        \"transport\": \"sse\",\n        \"url\": MCP_WEATHER_SERVER_URL,\n    }\n})\n\n# Step 2: Get tools from the MCP server (async -> await)\ntools = await client.get_tools()\n\n# Step 3: Show what we got\nprint(f\"\\nğŸ“¦ Loaded {len(tools)} tools from MCP server:\")\nfor tool in tools:\n    print(f\"   - {tool.name}: {tool.description}\")\n\nprint(\"\\nâœ… MCP tools configured successfully!\")\n\n# We keep 'client' alive so tools can work later\nmcp_client = client"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LangChain Agent\n",
    "\n",
    "Use the new `create_agent` API from LangChain 1.0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Creating LangChain agent...\n",
      "\n",
      "âœ… Agent created successfully!\n",
      "\n",
      "ğŸ“Š Agent Configuration:\n",
      "   Model: maas/Llama-4-Scout-17B-16E-W4A16\n",
      "   Tools: 1 MCP tools available\n",
      "   Framework: LangChain 1.0 (create_agent)\n"
     ]
    }
   ],
   "source": [
    "# === Create Agent using LangChain v1.0 ===\n",
    "print(\"ğŸ¤– Creating LangChain agent...\\n\")\n",
    "\n",
    "# Create agent with proper configuration\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=tools,\n",
    "    system_prompt=\"\"\"\n",
    "You are a helpful weather assistant powered by Llama Stack.\n",
    "\n",
    "You have access to weather tools that can retrieve current weather information.\n",
    "When a user asks about weather, use the available tools to get accurate data.\n",
    "\n",
    "Always:\n",
    "- Be concise and friendly\n",
    "- Use tools when needed to get real data\n",
    "- Provide clear, actionable information\n",
    "\"\"\".strip(),\n",
    ")\n",
    "\n",
    "print(\"âœ… Agent created successfully!\")\n",
    "print(\"\\nğŸ“Š Agent Configuration:\")\n",
    "print(f\"   Model: {INFERENCE_MODEL}\")\n",
    "print(f\"   Tools: {len(tools)} MCP tools available\")\n",
    "print(f\"   Framework: LangChain 1.0 (create_agent)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Agent\n",
    "\n",
    "Let's test the agent with a weather query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸš€ Testing LangChain 1.0 Agent with MCP Tools\n",
      "============================================================\n",
      "\n",
      "ğŸ‘¤ User: What is the weather in Miami?\n",
      "\n",
      "ğŸ¤– Agent Execution Trace:\n",
      "------------------------------------------------------------\n",
      "\n",
      "ğŸ‘¤ Human: What is the weather in Miami?\n",
      "\n",
      "ğŸ¤– AI (Tool Call):\n",
      "   Tool Calling: getforecast\n",
      "   Args: {'latitude': '25.7617', 'longitude': '-80.1918'}\n",
      "\n",
      "ğŸ› ï¸  Tool Result: Forecast for 25.7617, -80.1918:\n",
      "\n",
      "Tonight:\n",
      "Temperature: 76Â°F\n",
      "Wind: 6 mph S\n",
      "Slight Chance Showers And Thunderstorms\n",
      "---\n",
      "Tuesday:\n",
      "Temperature: 86Â°F\n",
      "Wind: 5 to 8 mph N\n",
      "Sunny\n",
      "---\n",
      "Tuesday Night:\n",
      "Temperature: 75Â°F\n",
      "Wind: 10 mph N\n",
      "Partly Cloudy\n",
      "---\n",
      "Wednesday:\n",
      "Temperature: 84Â°F\n",
      "Wind: 10 mph N\n",
      "Sunny\n",
      "---\n",
      "Wednesday Night:\n",
      "Temperature: 71Â°F\n",
      "Wind: 9 to 13 mph NW\n",
      "Mostly Clear\n",
      "---\n",
      "Thursday:\n",
      "Temperature: 81Â°F\n",
      "Wind: 13 to 16 mph NW\n",
      "Mostly Sunny\n",
      "---\n",
      "Thursday Night:\n",
      "Temperature: 66Â°F\n",
      "Wind: 10 to 15 mph NW\n",
      "Mostly Clear\n",
      "---\n",
      "Friday:\n",
      "Temperature: 77Â°F\n",
      "Wind: 7 to 10 mph N\n",
      "Sunny\n",
      "---\n",
      "Friday Night:\n",
      "Temperature: 67Â°F\n",
      "Wind: 7 mph N\n",
      "Mostly Clear\n",
      "---\n",
      "Saturday:\n",
      "Temperature: 79Â°F\n",
      "Wind: 7 mph NE\n",
      "Sunny\n",
      "---\n",
      "Saturday Night:\n",
      "Temperature: 69Â°F\n",
      "Wind: 7 mph NE\n",
      "Mostly Clear\n",
      "---\n",
      "Sunday:\n",
      "Temperature: 81Â°F\n",
      "Wind: 7 to 10 mph NW\n",
      "Sunny\n",
      "---\n",
      "Sunday Night:\n",
      "Temperature: 70Â°F\n",
      "Wind: 10 mph NW\n",
      "Mostly Clear\n",
      "---\n",
      "Monday:\n",
      "Temperature: 82Â°F\n",
      "Wind: 12 mph N\n",
      "Sunny\n",
      "---\n",
      "\n",
      "ğŸ¤– AI: The current weather in Miami is not provided, but the forecast is as follows:\n",
      "\n",
      "- Tonight: Slight chance of showers and thunderstorms, temperature 76Â°F, wind 6 mph S.\n",
      "- Tuesday: Sunny, temperature 86Â°F, wind 5 to 8 mph N.\n",
      "- Tuesday Night: Partly cloudy, temperature 75Â°F, wind 10 mph N.\n",
      "- Wednesday: Sunny, temperature 84Â°F, wind 10 mph N.\n",
      "- Wednesday Night: Mostly clear, temperature 71Â°F, wind 9 to 13 mph NW.\n",
      "- Thursday: Mostly sunny, temperature 81Â°F, wind 13 to 16 mph NW.\n",
      "- Thursday Night: Mostly clear, temperature 66Â°F, wind 10 to 15 mph NW.\n",
      "- Friday: Sunny, temperature 77Â°F, wind 7 to 10 mph N.\n",
      "- Friday Night: Mostly clear, temperature 67Â°F, wind 7 mph N.\n",
      "- Saturday: Sunny, temperature 79Â°F, wind 7 mph NE.\n",
      "- Saturday Night: Mostly clear, temperature 69Â°F, wind 7 mph NE.\n",
      "- Sunday: Sunny, temperature 81Â°F, wind 7 to 10 mph NW.\n",
      "- Sunday Night: Mostly clear, temperature 70Â°F, wind 10 mph NW.\n",
      "- Monday: Sunny, temperature 82Â°F, wind 12 mph N.\n",
      "\n",
      "============================================================\n",
      "âœ… Query completed successfully!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# === Test Agent ===\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸš€ Testing LangChain 1.0 Agent with MCP Tools\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Define async function to run agent\n",
    "async def run_agent_query(query: str):\n",
    "    \"\"\"\n",
    "    Run agent query and display results.\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ‘¤ User: {query}\\n\")\n",
    "    \n",
    "    # Invoke agent\n",
    "    result = await agent.ainvoke({\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": query}]\n",
    "    })\n",
    "    \n",
    "    # Display results\n",
    "    print(\"ğŸ¤– Agent Execution Trace:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for message in result[\"messages\"]:\n",
    "        if isinstance(message, HumanMessage):\n",
    "            print(f\"\\nğŸ‘¤ Human: {message.content}\")\n",
    "        \n",
    "        elif isinstance(message, AIMessage):\n",
    "            if hasattr(message, 'tool_calls') and message.tool_calls:\n",
    "                print(f\"\\nğŸ¤– AI (Tool Call):\")\n",
    "                for tool_call in message.tool_calls:\n",
    "                    print(f\"   Tool Calling: {tool_call['name']}\")\n",
    "                    print(f\"   Args: {tool_call['args']}\")\n",
    "            else:\n",
    "                print(f\"\\nğŸ¤– AI: {message.content}\")\n",
    "        \n",
    "        elif isinstance(message, ToolMessage):\n",
    "            print(f\"\\nğŸ› ï¸  Tool Result: {message.content}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ… Query completed successfully!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Run test query\n",
    "result = await run_agent_query(\"What is the weather in Miami?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "byo-agentic-framework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}