{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# LangChain Agents and Llama Stack with MCP Tools\n\nThis notebook demonstrates **LangChain 1.0** integration with **Llama Stack** responses API using proper **MCP (Model Context Protocol) adapters** for client-side tool execution.\n\n## Configuration\n\nThis notebook uses environment variables from `.env` file in the project root.\nCreate your own `.env` file based on `.env.example`.\n\n## Architecture\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚         LangChain 1.0 Agent                 â”‚\nâ”‚         (create_agent)                      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â”‚                    \n         â”‚ Model Calls        \n         â–¼                    \nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Llama Stack    â”‚   â”‚  MCP Client          â”‚\nâ”‚  (OpenAI API)   â”‚   â”‚  (langchain-mcp)     â”‚\nâ”‚  - vLLM Engine  â”‚â”€â”€â–¶â”‚  - SSE Transport     â”‚\nâ”‚  - Inference    â”‚   â”‚  - Tool Adapters     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                             â”‚\n                      Tool Execution\n                             â–¼\n                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                      â”‚ MCP Server   â”‚\n                      â”‚ (Weather)    â”‚\n                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Packages\n",
    "\n",
    "Install LangChain 1.0 and MCP adapters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.12 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -q \"langchain>=1.0\" \"langchain-openai>=0.3.32\" \"langchain-core>=0.3.75\" \"langchain-mcp-adapters>=0.1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Loading environment from: /Users/rcarrata/Code/AI_BU/byo-agentic-framework/.env\n",
      "âœ… .env file found\n",
      "âœ… All dependencies imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import sys\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain 1.0 imports\n",
    "from langchain.agents import create_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# MCP Adapters for client-side tool execution\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "# Load environment variables from project root\n",
    "project_root = Path.cwd().parent.parent if 'assets/notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "env_path = project_root / '.env'\n",
    "load_dotenv(env_path)\n",
    "\n",
    "print(f\"ğŸ“ Loading environment from: {env_path}\")\n",
    "print(f\"âœ… .env file {'found' if env_path.exists() else 'NOT FOUND'}\")\n",
    "print(\"âœ… All dependencies imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Llama Stack Connection\n",
    "\n",
    "Connect to Llama Stack's OpenAI-compatible endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ Llama Stack Configuration:\n",
      "   Endpoint using Responses API: https://llama-stack-lls-demo.apps.dev.rhoai.rh-aiservices-bu.com/v1/openai/v1\n",
      "   Model: Llama-4-Scout-17B-16E-W4A16\n",
      "\n",
      "ğŸ§ª Testing connectivity...\n",
      "ğŸ“¥ LLM Response: Connection successful\n",
      "ğŸ“¥ LLM Response metadata: {'token_usage': {'completion_tokens': 3, 'prompt_tokens': 21, 'total_tokens': 24, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'llama-4-scout-17b-16e-w4a16', 'system_fingerprint': None, 'id': 'chatcmpl-3a3db3b4d9c44f7f829c4dd2e8a3b2c6', 'finish_reason': 'stop', 'logprobs': None}\n",
      "âœ… Llama Stack connection successful!\n"
     ]
    }
   ],
   "source": [
    "# === Llama Stack Configuration ===\n",
    "# Load from environment variables\n",
    "LLAMA_STACK_OPENAI_ENDPOINT = os.getenv(\"LLAMA_STACK_OPENAI_ENDPOINT\")\n",
    "INFERENCE_MODEL = os.getenv(\"INFERENCE_MODEL\")\n",
    "API_KEY = os.getenv(\"API_KEY\", \"fake\")\n",
    "\n",
    "print(\"ğŸŒ Llama Stack Configuration:\")\n",
    "print(f\"   Endpoint using Responses API: {LLAMA_STACK_OPENAI_ENDPOINT}\")\n",
    "print(f\"   Model: {INFERENCE_MODEL}\")\n",
    "\n",
    "# Create ChatOpenAI client pointing to Llama Stack\n",
    "llm = ChatOpenAI(\n",
    "    model=INFERENCE_MODEL,\n",
    "    api_key=API_KEY,\n",
    "    base_url=LLAMA_STACK_OPENAI_ENDPOINT,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ§ª Testing connectivity...\")\n",
    "try:\n",
    "    response = llm.invoke(\"Say 'Connection successful' if you can read this.\")\n",
    "    print(f\"ğŸ“¥ LLM Response: {response.content}\")\n",
    "    print(f\"ğŸ“¥ LLM Response metadata: {response.response_metadata}\")\n",
    "    print(\"âœ… Llama Stack connection successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Connection failed: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up MCP Client and Tools\n",
    "\n",
    "Configure the MCP client to connect to the weather service using SSE transport:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ› ï¸  Configuring MCP client...\n",
      "\n",
      "ğŸŒ¦ï¸  MCP Server URL: http://mcp-weather-lls-demo.apps.dev.rhoai.rh-aiservices-bu.com/sse\n",
      "\n",
      "ğŸ“¦ Loaded 1 tools from MCP server:\n",
      "   - getforecast: Get real time weather forecast for a location\n",
      "\n",
      "âœ… MCP tools configured successfully!\n"
     ]
    }
   ],
   "source": [
    "# === MCP Client Configuration ===\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "# Load MCP server URL from environment\n",
    "MCP_WEATHER_SERVER_URL = os.getenv(\"MCP_WEATHER_SERVER_URL\")\n",
    "\n",
    "print(\"ğŸ› ï¸  Configuring MCP client...\\n\")\n",
    "print(f\"ğŸŒ¦ï¸  MCP Server URL: {MCP_WEATHER_SERVER_URL}\")\n",
    "\n",
    "# Step 1: Create the MCP client\n",
    "client = MultiServerMCPClient({\n",
    "    \"weather\": {\n",
    "        \"transport\": \"sse\",\n",
    "        \"url\": MCP_WEATHER_SERVER_URL,\n",
    "    }\n",
    "})\n",
    "\n",
    "# Step 2: Get tools from the MCP server (async -> await)\n",
    "tools = await client.get_tools()\n",
    "\n",
    "# Step 3: Show what we got\n",
    "print(f\"\\nğŸ“¦ Loaded {len(tools)} tools from MCP server:\")\n",
    "for tool in tools:\n",
    "    print(f\"   - {tool.name}: {tool.description}\")\n",
    "\n",
    "print(\"\\nâœ… MCP tools configured successfully!\")\n",
    "\n",
    "# We keep 'client' alive so tools can work later\n",
    "mcp_client = client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LangChain Agent\n",
    "\n",
    "Use the new `create_agent` API from LangChain 1.0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Creating LangChain agent...\n",
      "\n",
      "âœ… Agent created successfully!\n",
      "\n",
      "ğŸ“Š Agent Configuration:\n",
      "   Model: Llama-4-Scout-17B-16E-W4A16\n",
      "   Tools: 1 MCP tools available\n",
      "   Framework: LangChain 1.0 (create_agent)\n"
     ]
    }
   ],
   "source": [
    "# === Create Agent using LangChain v1.0 ===\n",
    "print(\"ğŸ¤– Creating LangChain agent...\\n\")\n",
    "\n",
    "# Create agent with proper configuration\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=tools,\n",
    "    system_prompt=\"\"\"\n",
    "You are a helpful weather assistant powered by Llama Stack.\n",
    "\n",
    "You have access to weather tools that can retrieve current weather information.\n",
    "When a user asks about weather, use the available tools to get accurate data.\n",
    "\n",
    "Always:\n",
    "- Be concise and friendly\n",
    "- Use tools when needed to get real data\n",
    "- Provide clear, actionable information\n",
    "\"\"\".strip(),\n",
    ")\n",
    "\n",
    "print(\"âœ… Agent created successfully!\")\n",
    "print(\"\\nğŸ“Š Agent Configuration:\")\n",
    "print(f\"   Model: {INFERENCE_MODEL}\")\n",
    "print(f\"   Tools: {len(tools)} MCP tools available\")\n",
    "print(f\"   Framework: LangChain 1.0 (create_agent)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Agent\n",
    "\n",
    "Let's test the agent with a weather query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸš€ Testing LangChain 1.0 Agent with MCP Tools\n",
      "============================================================\n",
      "\n",
      "ğŸ‘¤ User: What is the weather in Miami?\n",
      "\n",
      "ğŸ¤– Agent Execution Trace:\n",
      "------------------------------------------------------------\n",
      "\n",
      "ğŸ‘¤ Human: What is the weather in Miami?\n",
      "\n",
      "ğŸ¤– AI (Tool Call):\n",
      "   Tool Calling: getforecast\n",
      "   Args: {'latitude': '25.7617', 'longitude': '-80.1918'}\n",
      "\n",
      "ğŸ› ï¸  Tool Result: Forecast for 25.7617, -80.1918:\n",
      "\n",
      "Today:\n",
      "Temperature: 80Â°F\n",
      "Wind: 14 to 17 mph NW\n",
      "Sunny\n",
      "---\n",
      "Tonight:\n",
      "Temperature: 66Â°F\n",
      "Wind: 9 to 13 mph NW\n",
      "Mostly Clear\n",
      "---\n",
      "Friday:\n",
      "Temperature: 78Â°F\n",
      "Wind: 6 to 9 mph N\n",
      "Sunny\n",
      "---\n",
      "Friday Night:\n",
      "Temperature: 68Â°F\n",
      "Wind: 7 mph N\n",
      "Mostly Clear\n",
      "---\n",
      "Saturday:\n",
      "Temperature: 79Â°F\n",
      "Wind: 8 mph NE\n",
      "Sunny\n",
      "---\n",
      "Saturday Night:\n",
      "Temperature: 72Â°F\n",
      "Wind: 9 mph NE\n",
      "Partly Cloudy\n",
      "---\n",
      "Sunday:\n",
      "Temperature: 82Â°F\n",
      "Wind: 9 to 14 mph NE\n",
      "Mostly Sunny then Slight Chance Showers And Thunderstorms\n",
      "---\n",
      "Sunday Night:\n",
      "Temperature: 73Â°F\n",
      "Wind: 13 mph NE\n",
      "Slight Chance Showers And Thunderstorms\n",
      "---\n",
      "Monday:\n",
      "Temperature: 83Â°F\n",
      "Wind: 15 mph NE\n",
      "Slight Chance Showers And Thunderstorms\n",
      "---\n",
      "Monday Night:\n",
      "Temperature: 69Â°F\n",
      "Wind: 14 mph N\n",
      "Slight Chance Showers And Thunderstorms\n",
      "---\n",
      "Tuesday:\n",
      "Temperature: 82Â°F\n",
      "Wind: 15 mph N\n",
      "Slight Chance Showers And Thunderstorms then Mostly Sunny\n",
      "---\n",
      "Tuesday Night:\n",
      "Temperature: 72Â°F\n",
      "Wind: 14 mph N\n",
      "Partly Cloudy\n",
      "---\n",
      "Wednesday:\n",
      "Temperature: 82Â°F\n",
      "Wind: 16 mph NE\n",
      "Mostly Sunny then Slight Chance Showers And Thunderstorms\n",
      "---\n",
      "Wednesday Night:\n",
      "Temperature: 75Â°F\n",
      "Wind: 15 mph NE\n",
      "Slight Chance Showers And Thunderstorms then Partly Cloudy\n",
      "---\n",
      "\n",
      "ğŸ¤– AI: The current weather in Miami is mostly sunny with a temperature of 80Â°F and wind blowing at 14 to 17 mph NW.\n",
      "\n",
      "============================================================\n",
      "âœ… Query completed successfully!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# === Test Agent ===\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸš€ Testing LangChain 1.0 Agent with MCP Tools\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Define async function to run agent\n",
    "async def run_agent_query(query: str):\n",
    "    \"\"\"\n",
    "    Run agent query and display results.\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ‘¤ User: {query}\\n\")\n",
    "    \n",
    "    # Invoke agent\n",
    "    result = await agent.ainvoke({\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": query}]\n",
    "    })\n",
    "    \n",
    "    # Display results\n",
    "    print(\"ğŸ¤– Agent Execution Trace:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for message in result[\"messages\"]:\n",
    "        if isinstance(message, HumanMessage):\n",
    "            print(f\"\\nğŸ‘¤ Human: {message.content}\")\n",
    "        \n",
    "        elif isinstance(message, AIMessage):\n",
    "            if hasattr(message, 'tool_calls') and message.tool_calls:\n",
    "                print(f\"\\nğŸ¤– AI (Tool Call):\")\n",
    "                for tool_call in message.tool_calls:\n",
    "                    print(f\"   Tool Calling: {tool_call['name']}\")\n",
    "                    print(f\"   Args: {tool_call['args']}\")\n",
    "            else:\n",
    "                print(f\"\\nğŸ¤– AI: {message.content}\")\n",
    "        \n",
    "        elif isinstance(message, ToolMessage):\n",
    "            print(f\"\\nğŸ› ï¸  Tool Result: {message.content}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ… Query completed successfully!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Run test query\n",
    "result = await run_agent_query(\"What is the weather in Miami?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "byo-agentic-framework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}