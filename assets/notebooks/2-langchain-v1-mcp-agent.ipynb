{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Agents and Llama Stack with MCP Tools\n",
    "\n",
    "This notebook demonstrates **LangChain 1.0** integration with **Llama Stack** responses API using proper **MCP (Model Context Protocol) adapters** for client-side tool execution.\n",
    "\n",
    "## Configuration\n",
    "\n",
    "This notebook uses environment variables from `.env` file in the project root.\n",
    "Create your own `.env` file based on `.env.example`.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ         LangChain 1.0 Agent                 ‚îÇ\n",
    "‚îÇ         (create_agent)                      ‚îÇ\n",
    "‚îÇ         + MCP Client Adapters               ‚îÇ\n",
    "‚îÇ         (langchain-mcp)                     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚îÇ                    \n",
    "         ‚îÇ Model Calls        \n",
    "         ‚ñº                    \n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   \n",
    "‚îÇ  Llama Stack    ‚îÇ   \n",
    "‚îÇ  Responses API  ‚îÇ‚îÄ‚îÄ‚ñ∂ Tool Execution\n",
    "‚îÇ  - vLLM Engine  ‚îÇ         ‚îÇ\n",
    "‚îÇ  - Inference    ‚îÇ         ‚ñº\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                      ‚îÇ MCP Server   ‚îÇ\n",
    "                      ‚îÇ (Weather)    ‚îÇ\n",
    "                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Packages\n",
    "\n",
    "Install LangChain 1.0 and MCP adapters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q \"langchain>=1.0\" \"langchain-openai>=0.3.32\" \"langchain-core>=0.3.75\" \"langchain-mcp-adapters>=0.1.0\" \"llama-stack-client==0.3.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import sys\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# LangChain 1.0 imports\n",
    "from langchain.agents import create_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# MCP Adapters for client-side tool execution\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "# --- Load environment variables ---\n",
    "# Automatically detect the nearest .env (walks up from current directory)\n",
    "env_path = find_dotenv(usecwd=True)\n",
    "if env_path:\n",
    "    load_dotenv(env_path)\n",
    "    print(f\"üìÅ Loading environment from: {env_path}\")\n",
    "    print(\"‚úÖ .env file FOUND and loaded\")\n",
    "else:\n",
    "    default_path = Path.cwd() / \".env\"\n",
    "    print(f\"üìÅ No .env found via find_dotenv ‚Äî checked: {default_path}\")\n",
    "    print(\"‚ö†Ô∏è  .env file NOT FOUND\")\n",
    "\n",
    "# --- Verify Python interpreter / kernel ---\n",
    "print(f\"\\nüêç Python: {sys.executable}\")\n",
    "\n",
    "# Detect if running inside a virtual environment\n",
    "in_venv = (\n",
    "    hasattr(sys, \"real_prefix\") or\n",
    "    (getattr(sys, \"base_prefix\", sys.prefix) != sys.prefix) or\n",
    "    \"VIRTUAL_ENV\" in os.environ or\n",
    "    \"CONDA_PREFIX\" in os.environ\n",
    ")\n",
    "\n",
    "if in_venv:\n",
    "    print(\"‚úÖ Using virtual environment - CORRECT!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Using global Python - Consider switching kernel!\")\n",
    "    print(\"   Click 'Select Kernel' ‚Üí Choose 'Python (byo-agentic-framework)')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Llama Stack Connection\n",
    "\n",
    "Connect to Llama Stack's OpenAI-compatible endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Llama Stack Configuration ===\n",
    "# Load from environment variables\n",
    "LLAMA_STACK_OPENAI_ENDPOINT = os.getenv(\"LLAMA_STACK_OPENAI_ENDPOINT\")\n",
    "INFERENCE_MODEL = os.getenv(\"INFERENCE_MODEL\")\n",
    "API_KEY = os.getenv(\"API_KEY\", \"fake\")\n",
    "\n",
    "print(\"üåê Llama Stack Configuration:\")\n",
    "print(f\"   Endpoint using Responses API: {LLAMA_STACK_OPENAI_ENDPOINT}\")\n",
    "print(f\"   Model: {INFERENCE_MODEL}\")\n",
    "\n",
    "# Create ChatOpenAI client pointing to Llama Stack\n",
    "llm = ChatOpenAI(\n",
    "    model=INFERENCE_MODEL,\n",
    "    api_key=API_KEY,\n",
    "    base_url=LLAMA_STACK_OPENAI_ENDPOINT,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "print(\"\\nüß™ Testing connectivity...\")\n",
    "try:\n",
    "    response = llm.invoke(\"Say 'Connection successful' if you can read this.\")\n",
    "    print(f\"üì• LLM Response: {response.content}\")\n",
    "    print(f\"üì• LLM Response metadata: {response.response_metadata}\")\n",
    "    print(\"‚úÖ Llama Stack connection successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection failed: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up MCP Client and Tools\n",
    "\n",
    "Configure the MCP client to connect to the weather service using SSE transport:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MCP Client Configuration ===\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "# Load MCP server URL from environment\n",
    "MCP_WEATHER_SERVER_URL = os.getenv(\"MCP_WEATHER_SERVER_URL\")\n",
    "\n",
    "print(\"üõ†Ô∏è  Configuring MCP client...\\n\")\n",
    "print(f\"üå¶Ô∏è  MCP Server URL: {MCP_WEATHER_SERVER_URL}\")\n",
    "\n",
    "# Step 1: Create the MCP client\n",
    "client = MultiServerMCPClient({\n",
    "    \"weather\": {\n",
    "        \"transport\": \"sse\",\n",
    "        \"url\": MCP_WEATHER_SERVER_URL,\n",
    "    }\n",
    "})\n",
    "\n",
    "# Step 2: Get tools from the MCP server (async -> await)\n",
    "tools = await client.get_tools()\n",
    "\n",
    "# Step 3: Show what we got\n",
    "print(f\"\\nüì¶ Loaded {len(tools)} tools from MCP server:\")\n",
    "for tool in tools:\n",
    "    print(f\"   - {tool.name}: {tool.description}\")\n",
    "\n",
    "print(\"\\n‚úÖ MCP tools configured successfully!\")\n",
    "\n",
    "# We keep 'client' alive so tools can work later\n",
    "mcp_client = client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LangChain Agent\n",
    "\n",
    "Use the new `create_agent` API from LangChain 1.0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Create Agent using LangChain v1.0 ===\n",
    "print(\"ü§ñ Creating LangChain agent...\\n\")\n",
    "\n",
    "# Create agent with proper configuration\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=tools,\n",
    "    system_prompt=\"\"\"\n",
    "You are a helpful weather assistant powered by Llama Stack.\n",
    "\n",
    "You have access to weather tools that can retrieve current weather information.\n",
    "When a user asks about weather, use the available tools to get accurate data.\n",
    "\n",
    "Always:\n",
    "- Be concise and friendly\n",
    "- Use tools when needed to get real data\n",
    "- Provide clear, actionable information\n",
    "\"\"\".strip(),\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Agent created successfully!\")\n",
    "print(\"\\nüìä Agent Configuration:\")\n",
    "print(f\"   Model: {INFERENCE_MODEL}\")\n",
    "print(f\"   Tools: {len(tools)} MCP tools available\")\n",
    "print(f\"   Framework: LangChain 1.0 (create_agent)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Agent\n",
    "\n",
    "Let's test the agent with a weather query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Test Agent ===\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ Testing LangChain 1.0 Agent with MCP Tools\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Define async function to run agent\n",
    "async def run_agent_query(query: str):\n",
    "    \"\"\"\n",
    "    Run agent query and display results.\n",
    "    \"\"\"\n",
    "    print(f\"üë§ User: {query}\\n\")\n",
    "    \n",
    "    # Invoke agent\n",
    "    result = await agent.ainvoke({\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": query}]\n",
    "    })\n",
    "    \n",
    "    # Display results\n",
    "    print(\"ü§ñ Agent Execution Trace:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for message in result[\"messages\"]:\n",
    "        if isinstance(message, HumanMessage):\n",
    "            print(f\"\\nüë§ Human: {message.content}\")\n",
    "        \n",
    "        elif isinstance(message, AIMessage):\n",
    "            if hasattr(message, 'tool_calls') and message.tool_calls:\n",
    "                print(f\"\\nü§ñ AI (Tool Call):\")\n",
    "                for tool_call in message.tool_calls:\n",
    "                    print(f\"   Tool Calling: {tool_call['name']}\")\n",
    "                    print(f\"   Args: {tool_call['args']}\")\n",
    "            else:\n",
    "                print(f\"\\nü§ñ AI: {message.content}\")\n",
    "        \n",
    "        elif isinstance(message, ToolMessage):\n",
    "            print(f\"\\nüõ†Ô∏è  Tool Result: {message.content}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ Query completed successfully!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Run test query\n",
    "result = await run_agent_query(\"What is the weather in Miami?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "byo-agentic-framework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
