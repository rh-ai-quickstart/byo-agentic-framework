{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with Llama Stack Primitives\n",
    "\n",
    "This notebook demonstrates building a **Retrieval-Augmented Generation (RAG)** system using **pure Llama Stack primitives** - no frameworks like LangChain or CrewAI.\n",
    "\n",
    "## Agentic RAG Approach\n",
    "\n",
    "This notebook shows the **Agentic RAG loop**:\n",
    "\n",
    "1. **Query** â†’ User asks a question\n",
    "2. **Search** â†’ Semantic search in vector store\n",
    "3. **Extract** â†’ Get relevant document chunks\n",
    "4. **Generate** â†’ Pass context + query to LLM\n",
    "5. **Answer** â†’ Return generated response\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         User Query                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Vector Search                              â”‚\n",
    "â”‚  (client.vector_stores.search)              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Context Extraction                         â”‚\n",
    "â”‚  (Extract relevant chunks)                  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Llama Stack Responses API                  â”‚\n",
    "â”‚  (client.responses.create)                  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         Final Answer                        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## Configuration\n",
    "\n",
    "This notebook uses environment variables from `.env` file in the project root.\n",
    "Create your own `.env` file based on `.env.example`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Packages\n",
    "\n",
    "Install Llama Stack client (version must match server version):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -q \"llama-stack-client==0.2.22\" \"python-dotenv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from io import BytesIO\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Llama Stack client\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "# Load environment variables from project root\n",
    "project_root = Path.cwd().parent.parent if 'assets/notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "env_path = project_root / '.env'\n",
    "load_dotenv(env_path)\n",
    "\n",
    "print(f\"ğŸ“ Loading environment from: {env_path}\")\n",
    "print(f\"âœ… .env file {'found' if env_path.exists() else 'NOT FOUND'}\")\n",
    "\n",
    "# Verify kernel\n",
    "print(f\"\\nğŸ Python: {sys.executable}\")\n",
    "if \".venv\" in sys.executable:\n",
    "    print(\"âœ… Using project venv - CORRECT!\")\n",
    "else:\n",
    "    print(\"âš ï¸  Using global Python - Consider switching kernel!\")\n",
    "    print(\"   Click 'Select Kernel' â†’ Choose 'Python (byo-agentic-framework)')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Llama Stack Connection\n",
    "\n",
    "Create client using Llama Stack's base URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get configuration from environment variables\n",
    "LLAMA_STACK_BASE_URL = os.getenv(\"LLAMA_STACK_BASE_URL\")\n",
    "INFERENCE_MODEL = os.getenv(\"INFERENCE_MODEL\")\n",
    "LLAMA_STACK_OPENAI_ENDPOINT = os.getenv(\"LLAMA_STACK_OPENAI_ENDPOINT\")\n",
    "\n",
    "print(\"ğŸŒ Llama Stack Configuration:\")\n",
    "print(f\"   Base URL: {LLAMA_STACK_BASE_URL}\")\n",
    "print(f\"   Model: {INFERENCE_MODEL}\")\n",
    "print(f\"   OpenAI Endpoint: {LLAMA_STACK_OPENAI_ENDPOINT}\")\n",
    "\n",
    "# Create client using the base URL\n",
    "client = LlamaStackClient(base_url=LLAMA_STACK_BASE_URL)\n",
    "print(\"\\nâœ… Llama Stack client created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store Setup\n",
    "\n",
    "Create a vector store with sample Acme company documentation.\n",
    "\n",
    "### Step 1: Prepare Documents\n",
    "\n",
    "We'll use three simple documents about company policies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“š Preparing Acme documentation...\\n\")\n",
    "\n",
    "# Sample documents for RAG\n",
    "docs = [\n",
    "    (\"Acme ships globally in 3-5 business days.\", {\"title\": \"Shipping Policy\"}),\n",
    "    (\"Returns are accepted within 30 days of purchase.\", {\"title\": \"Returns Policy\"}),\n",
    "    (\"Support is available 24/7 via chat and email.\", {\"title\": \"Support\"}),\n",
    "]\n",
    "\n",
    "print(\"ğŸ“„ Documents to upload:\")\n",
    "for content, metadata in docs:\n",
    "    print(f\"   â€¢ {metadata['title']}: {content}\")\n",
    "\n",
    "print(f\"\\nâœ… {len(docs)} documents prepared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Upload Files to Llama Stack\n",
    "\n",
    "Upload documents using the Files API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ids = []\n",
    "print(\"ğŸ“¤ Uploading documents to Llama Stack...\\n\")\n",
    "\n",
    "for content, metadata in docs:\n",
    "    # Create in-memory file from document text\n",
    "    with BytesIO(content.encode()) as file_buffer:\n",
    "        # Set filename for the buffer\n",
    "        file_buffer.name = f\"{metadata['title'].replace(' ', '_').lower()}.txt\"\n",
    "        \n",
    "        # Upload to Llama Stack\n",
    "        file_response = client.files.create(\n",
    "            file=file_buffer,\n",
    "            purpose=\"assistants\"\n",
    "        )\n",
    "        \n",
    "        print(f\"   âœ… Uploaded: {metadata['title']}\")\n",
    "        print(f\"      File ID: {file_response.id}\")\n",
    "        \n",
    "        file_ids.append(file_response.id)\n",
    "\n",
    "print(f\"\\nâœ… All {len(file_ids)} files uploaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create Vector Store\n",
    "\n",
    "Create a vector store with the uploaded files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”§ Creating vector store...\\n\")\n",
    "\n",
    "vector_store = client.vector_stores.create(\n",
    "    name=\"acme_docs\",\n",
    "    file_ids=file_ids,\n",
    "    embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    embedding_dimension=384,\n",
    "    provider_id=\"milvus\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Vector store created successfully!\")\n",
    "print(f\"   ID: {vector_store.id}\")\n",
    "print(f\"   Name: {vector_store.name}\")\n",
    "print(f\"   Embedding Model: sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(f\"   Dimensions: 384\")\n",
    "print(f\"   Provider: Milvus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Direct Vector Search\n",
    "\n",
    "Test semantic search by querying the vector store directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ” Testing vector search...\\n\")\n",
    "\n",
    "test_query = \"How long does shipping take?\"\n",
    "print(f\"Query: {test_query}\\n\")\n",
    "\n",
    "# Perform semantic search\n",
    "search_response = client.vector_stores.search(\n",
    "    vector_store_id=vector_store.id,\n",
    "    query=test_query,\n",
    "    max_num_results=2\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"ğŸ“Š Search Results:\\n\")\n",
    "for i, result in enumerate(search_response.data, 1):\n",
    "    content = result.content[0].text\n",
    "    filename = result.filename if hasattr(result, 'filename') else 'Unknown'\n",
    "    print(f\"   [{i}] {filename}\")\n",
    "    print(f\"       Content: {content}\\n\")\n",
    "\n",
    "print(\"âœ… Vector search working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agentic RAG Function\n",
    "\n",
    "Define a function that implements the Agentic RAG loop:\n",
    "\n",
    "1. Search the vector store for relevant chunks\n",
    "2. Extract context from search results\n",
    "3. Build a prompt with context + query\n",
    "4. Call Responses API to generate answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agentic_rag(client, vector_store_id, query, model, max_results=3):\n",
    "    \"\"\"\n",
    "    Perform Agentic RAG: search â†’ extract context â†’ generate answer\n",
    "    \n",
    "    Args:\n",
    "        client: LlamaStackClient instance\n",
    "        vector_store_id: ID of the vector store to search\n",
    "        query: User's question\n",
    "        model: Model ID for inference\n",
    "        max_results: Number of chunks to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        dict with query, context, answer, and metadata\n",
    "    \"\"\"\n",
    "    # 1. Search vector store for relevant chunks\n",
    "    search_results = client.vector_stores.search(\n",
    "        vector_store_id=vector_store_id,\n",
    "        query=query,\n",
    "        max_num_results=max_results\n",
    "    )\n",
    "    \n",
    "    # 2. Extract context from search results using traditional loop\n",
    "    context_chunks = []\n",
    "    for result in search_results.data:\n",
    "        text = result.content[0].text\n",
    "        context_chunks.append(text)\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_chunks)\n",
    "    \n",
    "    # 3. Build prompt with context\n",
    "    prompt = f\"\"\"Use the following context to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer based only on the context provided above.\"\"\"\n",
    "    \n",
    "    # 4. Call Responses API to generate answer\n",
    "    response = client.responses.create(\n",
    "        model=model,\n",
    "        input=prompt\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"context\": context,\n",
    "        \"answer\": response.output_text,\n",
    "        \"num_chunks\": len(context_chunks)\n",
    "    }\n",
    "\n",
    "print(\"âœ… Agentic RAG function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Shipping Query\n",
    "\n",
    "Test the RAG system with a shipping-related question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸš€ Example 1: Shipping Query\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "query = \"How long does shipping take?\"\n",
    "print(f\"â“ Query: {query}\\n\")\n",
    "\n",
    "# Perform agentic RAG\n",
    "result = agentic_rag(\n",
    "    client=client,\n",
    "    vector_store_id=vector_store.id,\n",
    "    query=query,\n",
    "    model=INFERENCE_MODEL,\n",
    "    max_results=3\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(f\"ğŸ” Retrieved {result['num_chunks']} relevant chunks\\n\")\n",
    "print(\"ğŸ“„ Context:\")\n",
    "print(\"-\" * 60)\n",
    "print(result['context'])\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"\\nğŸ’¡ Answer:\")\n",
    "print(\"=\" * 60)\n",
    "print(result['answer'])\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Returns Policy Query\n",
    "\n",
    "Test with a question about return policies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸš€ Example 2: Returns Policy Query\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "query = \"Can I return a product after 40 days?\"\n",
    "print(f\"â“ Query: {query}\\n\")\n",
    "\n",
    "# Perform agentic RAG\n",
    "result = agentic_rag(\n",
    "    client=client,\n",
    "    vector_store_id=vector_store.id,\n",
    "    query=query,\n",
    "    model=INFERENCE_MODEL,\n",
    "    max_results=3\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(f\"ğŸ” Retrieved {result['num_chunks']} relevant chunks\\n\")\n",
    "print(\"ğŸ“„ Context:\")\n",
    "print(\"-\" * 60)\n",
    "print(result['context'])\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"\\nğŸ’¡ Answer:\")\n",
    "print(\"=\" * 60)\n",
    "print(result['answer'])\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Support Query\n",
    "\n",
    "Test with a question about customer support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸš€ Example 3: Support Query\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "query = \"How can I contact support?\"\n",
    "print(f\"â“ Query: {query}\\n\")\n",
    "\n",
    "# Perform agentic RAG\n",
    "result = agentic_rag(\n",
    "    client=client,\n",
    "    vector_store_id=vector_store.id,\n",
    "    query=query,\n",
    "    model=INFERENCE_MODEL,\n",
    "    max_results=3\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(f\"ğŸ” Retrieved {result['num_chunks']} relevant chunks\\n\")\n",
    "print(\"ğŸ“„ Context:\")\n",
    "print(\"-\" * 60)\n",
    "print(result['context'])\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"\\nğŸ’¡ Answer:\")\n",
    "print(\"=\" * 60)\n",
    "print(result['answer'])\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "We have successfully built an Agentic RAG system using **pure Llama Stack primitives and Responses API** - no frameworks needed!\n",
    "\n",
    "### What We Demonstrated:\n",
    "\n",
    "âœ… **Vector Store Creation** - Created a vector database with embedded documents  \n",
    "âœ… **Document Upload** - Uploaded files using the Files API  \n",
    "âœ… **Semantic Search** - Performed vector similarity search  \n",
    "âœ… **Context Extraction** - Retrieved relevant document chunks  \n",
    "âœ… **Response Generation** - Used Responses API for final answer  \n",
    "\n",
    "### Key Llama Stack Primitives Used:\n",
    "\n",
    "| API | Purpose | Method |\n",
    "|-----|---------|--------|\n",
    "| **Files API** | Upload documents | `client.files.create()` |\n",
    "| **Vector Stores API** | Create vector database | `client.vector_stores.create()` |\n",
    "| **Search API** | Semantic search | `client.vector_stores.search()` |\n",
    "| **Responses API** | Generate answers | `client.responses.create()` |\n",
    "\n",
    "### Agentic RAG Workflow:\n",
    "\n",
    "```\n",
    "1. User Query\n",
    "   â†“\n",
    "2. vector_stores.search() â†’ Find relevant chunks\n",
    "   â†“\n",
    "3. Extract context from search results\n",
    "   â†“\n",
    "4. Build prompt with context + query\n",
    "   â†“\n",
    "5. responses.create() â†’ Generate answer\n",
    "   â†“\n",
    "6. Return final answer\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "byo-agentic-framework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
