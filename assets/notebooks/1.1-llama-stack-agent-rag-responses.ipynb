{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with Llama Stack Primitives\n",
    "\n",
    "This notebook demonstrates building a **Retrieval-Augmented Generation (RAG)** system using **pure Llama Stack primitives** - no frameworks like LangChain or CrewAI.\n",
    "\n",
    "## Agentic RAG Approach\n",
    "\n",
    "This notebook shows the **Agentic RAG loop**:\n",
    "\n",
    "1. **Query** â†’ User asks a question\n",
    "2. **Search** â†’ Semantic search in vector store\n",
    "3. **Extract** â†’ Get relevant document chunks\n",
    "4. **Generate** â†’ Pass context + query to LLM\n",
    "5. **Answer** â†’ Return generated response\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         User Query                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Vector Search                              â”‚\n",
    "â”‚  (client.vector_stores.search)              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Context Extraction                         â”‚\n",
    "â”‚  (Extract relevant chunks)                  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Llama Stack Responses API                  â”‚\n",
    "â”‚  (client.responses.create)                  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         Final Answer                        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## Configuration\n",
    "\n",
    "This notebook uses environment variables from `.env` file in the project root.\n",
    "Create your own `.env` file based on `.env.example`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Packages\n",
    "\n",
    "Install Llama Stack client (version must match server version):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.12 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -q \"llama-stack-client==0.2.22\" \"python-dotenv\" \"requests\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Loading environment from: /Users/rcarrata/Code/AI_BU/byo-agentic-framework/.env\n",
      "âœ… .env file found\n",
      "\n",
      "ðŸ Python: /Users/rcarrata/Code/AI_BU/byo-agentic-framework/.venv/bin/python\n",
      "âœ… Using project venv - CORRECT!\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from io import BytesIO\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Llama Stack client\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "# Load environment variables from project root\n",
    "project_root = Path.cwd().parent.parent if 'assets/notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "env_path = project_root / '.env'\n",
    "load_dotenv(env_path)\n",
    "\n",
    "print(f\"ðŸ“ Loading environment from: {env_path}\")\n",
    "print(f\"âœ… .env file {'found' if env_path.exists() else 'NOT FOUND'}\")\n",
    "\n",
    "# Verify kernel\n",
    "print(f\"\\nðŸ Python: {sys.executable}\")\n",
    "if \".venv\" in sys.executable:\n",
    "    print(\"âœ… Using project venv - CORRECT!\")\n",
    "else:\n",
    "    print(\"âš ï¸  Using global Python - Consider switching kernel!\")\n",
    "    print(\"   Click 'Select Kernel' â†’ Choose 'Python (byo-agentic-framework)'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Llama Stack Connection\n",
    "\n",
    "Create client using Llama Stack's base URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ Llama Stack Configuration:\n",
      "   Base URL: https://llama-stack-lls-demo.apps.dev.rhoai.rh-aiservices-bu.com\n",
      "   Model: vllm-inference/llama32\n",
      "   OpenAI Endpoint: https://llama-stack-lls-demo.apps.dev.rhoai.rh-aiservices-bu.com/v1/openai/v1\n",
      "\n",
      "âœ… Llama Stack client created\n"
     ]
    }
   ],
   "source": [
    "# Get configuration from environment variables\n",
    "LLAMA_STACK_BASE_URL = os.getenv(\"LLAMA_STACK_BASE_URL\")\n",
    "INFERENCE_MODEL = os.getenv(\"INFERENCE_MODEL\")\n",
    "LLAMA_STACK_OPENAI_ENDPOINT = os.getenv(\"LLAMA_STACK_OPENAI_ENDPOINT\")\n",
    "\n",
    "print(\"ðŸŒ Llama Stack Configuration:\")\n",
    "print(f\"   Base URL: {LLAMA_STACK_BASE_URL}\")\n",
    "print(f\"   Model: {INFERENCE_MODEL}\")\n",
    "print(f\"   OpenAI Endpoint: {LLAMA_STACK_OPENAI_ENDPOINT}\")\n",
    "\n",
    "# Create client using the base URL\n",
    "client = LlamaStackClient(base_url=LLAMA_STACK_BASE_URL)\n",
    "print(\"\\nâœ… Llama Stack client created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store Setup\n",
    "\n",
    "Create a vector store with Parasol Cloud Corp earnings report.\n",
    "\n",
    "### Step 1: Download PDF Document\n",
    "\n",
    "We'll download the FY2025 earnings report from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“š Downloading Parasol Cloud Corp earnings report...\n",
      "\n",
      "ðŸ“¥ Downloading from: https://raw.githubusercontent.com/rh-ai-quickstart/byo-agentic-framework/dev/assets/images/Parasol_Cloud_Corp_Earnings_Report.pdf\n",
      "âœ… Downloaded 12296 bytes\n",
      "   Content type: application/pdf\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“š Downloading Parasol Cloud Corp earnings report...\\n\")\n",
    "\n",
    "# PDF document URL\n",
    "pdf_url = \"https://raw.githubusercontent.com/rh-ai-quickstart/byo-agentic-framework/dev/assets/images/Parasol_Cloud_Corp_Earnings_Report.pdf\"\n",
    "\n",
    "# Download the PDF\n",
    "print(f\"ðŸ“¥ Downloading from: {pdf_url}\")\n",
    "response = requests.get(pdf_url)\n",
    "response.raise_for_status()  # Raise error if download fails\n",
    "\n",
    "pdf_content = response.content\n",
    "print(f\"âœ… Downloaded {len(pdf_content)} bytes\")\n",
    "print(f\"   Content type: application/pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Upload PDF to Llama Stack\n",
    "\n",
    "Upload the earnings report PDF using the Files API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¤ Uploading PDF to Llama Stack...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://llama-stack-lls-demo.apps.dev.rhoai.rh-aiservices-bu.com/v1/openai/v1/files \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Uploaded: Parasol Cloud Corp FY2025 Earnings Report\n",
      "   File ID: file-dc8ff546fd3b4d08aa051a5c66ac3e8d\n",
      "   Filename: Parasol_Cloud_Corp_Earnings_Report.pdf\n",
      "\n",
      "âœ… File uploaded successfully\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“¤ Uploading PDF to Llama Stack...\\n\")\n",
    "\n",
    "# Create in-memory file from PDF content\n",
    "with BytesIO(pdf_content) as pdf_buffer:\n",
    "    # Set filename for the PDF\n",
    "    pdf_buffer.name = \"Parasol_Cloud_Corp_Earnings_Report.pdf\"\n",
    "    \n",
    "    # Upload to Llama Stack\n",
    "    file_response = client.files.create(\n",
    "        file=pdf_buffer,\n",
    "        purpose=\"assistants\"\n",
    "    )\n",
    "    \n",
    "    file_id = file_response.id\n",
    "    print(f\"âœ… Uploaded: Parasol Cloud Corp FY2025 Earnings Report\")\n",
    "    print(f\"   File ID: {file_id}\")\n",
    "    print(f\"   Filename: Parasol_Cloud_Corp_Earnings_Report.pdf\")\n",
    "\n",
    "file_ids = [file_id]\n",
    "print(f\"\\nâœ… File uploaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create Vector Store\n",
    "\n",
    "Create a vector store with the uploaded files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Creating vector store...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://llama-stack-lls-demo.apps.dev.rhoai.rh-aiservices-bu.com/v1/openai/v1/vector_stores \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Vector store created successfully!\n",
      "   ID: vs_8474850c-2bb6-476e-9ac6-6c5c5e999e0f\n",
      "   Name: parasol_earnings_fy2025\n",
      "   Embedding Model: sentence-transformers/all-MiniLM-L6-v2\n",
      "   Dimensions: 384\n",
      "   Provider: Milvus\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ”§ Creating vector store...\\n\")\n",
    "\n",
    "vector_store = client.vector_stores.create(\n",
    "    name=\"parasol_earnings_fy2025\",\n",
    "    file_ids=file_ids,\n",
    "    embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    embedding_dimension=384,\n",
    "    provider_id=\"milvus\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Vector store created successfully!\")\n",
    "print(f\"   ID: {vector_store.id}\")\n",
    "print(f\"   Name: {vector_store.name}\")\n",
    "print(f\"   Embedding Model: sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(f\"   Dimensions: 384\")\n",
    "print(f\"   Provider: Milvus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Direct Vector Search\n",
    "\n",
    "Test semantic search by querying the vector store directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Testing vector search...\n",
      "\n",
      "Query: What is the total revenue for FY2025?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://llama-stack-lls-demo.apps.dev.rhoai.rh-aiservices-bu.com/v1/openai/v1/vector_stores/vs_8474850c-2bb6-476e-9ac6-6c5c5e999e0f/search \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Search Results:\n",
      "\n",
      "   [1] \n",
      "       Content: Parasol Cloud Corp â€“ FY2025 Earnings Report\n",
      "Executive Summary\n",
      "Parasol Cloud Corp delivered strong performance in FY2025, achieving record revenue\n",
      "growth driven by subscription renewals and strategic u...\n",
      "\n",
      "   [2] \n",
      "       Content: ,000 (Tier A), $5,000\n",
      "(Cloud Upsell) - (2) Active | Account: Globex Inc | Items: $25,000 (Enterprise License\n",
      "Renewal) - (3) Closed | Account: Soylent Corp | Items: $8,000 (Legacy Support)\n",
      "Support Case...\n",
      "\n",
      "   [3] \n",
      "       Content:  of early, mid, and\n",
      "late-stage deals. The closed opportunities show a natural decline due to the phase-out\n",
      "of legacy contracts, aligning with corporate strategy.\n",
      "\n",
      "Support Case Analysis\n",
      "A total of 6 su...\n",
      "\n",
      "âœ… Vector search working correctly!\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ” Testing vector search...\\n\")\n",
    "\n",
    "test_query = \"What is the total revenue for FY2025?\"\n",
    "print(f\"Query: {test_query}\\n\")\n",
    "\n",
    "# Perform semantic search\n",
    "search_response = client.vector_stores.search(\n",
    "    vector_store_id=vector_store.id,\n",
    "    query=test_query,\n",
    "    max_num_results=3\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"ðŸ“Š Search Results:\\n\")\n",
    "for i, result in enumerate(search_response.data, 1):\n",
    "    content = result.content[0].text\n",
    "    filename = result.filename if hasattr(result, 'filename') else 'Unknown'\n",
    "    print(f\"   [{i}] {filename}\")\n",
    "    print(f\"       Content: {content[:200]}...\")  # Show first 200 chars\n",
    "    print()\n",
    "\n",
    "print(\"âœ… Vector search working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agentic RAG Function\n",
    "\n",
    "Define a function that implements the Agentic RAG loop:\n",
    "\n",
    "1. Search the vector store for relevant chunks\n",
    "2. Extract context from search results\n",
    "3. Build a prompt with context + query\n",
    "4. Call Responses API to generate answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Agentic RAG function defined\n"
     ]
    }
   ],
   "source": [
    "def agentic_rag(client, vector_store_id, query, model, max_results=3):\n",
    "    \"\"\"\n",
    "    Perform Agentic RAG: search â†’ extract context â†’ generate answer\n",
    "    \n",
    "    Args:\n",
    "        client: LlamaStackClient instance\n",
    "        vector_store_id: ID of the vector store to search\n",
    "        query: User's question\n",
    "        model: Model ID for inference\n",
    "        max_results: Number of chunks to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        dict with query, context, answer, and metadata\n",
    "    \"\"\"\n",
    "    # 1. Search vector store for relevant chunks\n",
    "    search_results = client.vector_stores.search(\n",
    "        vector_store_id=vector_store_id,\n",
    "        query=query,\n",
    "        max_num_results=max_results\n",
    "    )\n",
    "    \n",
    "    # 2. Extract context from search results using traditional loop\n",
    "    context_chunks = []\n",
    "    for result in search_results.data:\n",
    "        text = result.content[0].text\n",
    "        context_chunks.append(text)\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_chunks)\n",
    "    \n",
    "    # 3. Build prompt with context\n",
    "    prompt = f\"\"\"Use the following context to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer based only on the context provided above.\"\"\"\n",
    "    \n",
    "    # 4. Call Responses API to generate answer\n",
    "    response = client.responses.create(\n",
    "        model=model,\n",
    "        input=prompt\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"context\": context,\n",
    "        \"answer\": response.output_text,\n",
    "        \"num_chunks\": len(context_chunks)\n",
    "    }\n",
    "\n",
    "print(\"âœ… Agentic RAG function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Revenue Query\n",
    "\n",
    "Test the RAG system with a financial metrics question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸš€ Example 1: Revenue Query\n",
      "============================================================\n",
      "\n",
      "â“ Query: What is the total revenue of Parasol Cloud Corp in FY2025?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://llama-stack-lls-demo.apps.dev.rhoai.rh-aiservices-bu.com/v1/openai/v1/vector_stores/vs_8474850c-2bb6-476e-9ac6-6c5c5e999e0f/search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://llama-stack-lls-demo.apps.dev.rhoai.rh-aiservices-bu.com/v1/openai/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Retrieved 3 relevant chunks\n",
      "\n",
      "ðŸ“„ Context:\n",
      "------------------------------------------------------------\n",
      "Parasol Cloud Corp â€“ FY2025 Earnings Report\n",
      "Executive Summary\n",
      "Parasol Cloud Corp delivered strong performance in FY2025, achieving record revenue\n",
      "growth driven by subscription renewals and strategic upsells. The company maintained\n",
      "a healthy opportunity pipeline, with active opportunities contributing to a projected\n",
      "18% YoY growth. Customer engagement remains strong, with significant\n",
      "improvements in SLA compliance despite an increase in high-severity support cases.\n",
      "\n",
      "Financial Highlights\n",
      "Total rec...\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸ’¡ Answer:\n",
      "============================================================\n",
      "According to the context, the total recognized revenue for FY2025 reached $48M.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸš€ Example 1: Revenue Query\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "query = \"What is the total revenue of Parasol Cloud Corp in FY2025?\"\n",
    "print(f\"â“ Query: {query}\\n\")\n",
    "\n",
    "# Perform agentic RAG\n",
    "result = agentic_rag(\n",
    "    client=client,\n",
    "    vector_store_id=vector_store.id,\n",
    "    query=query,\n",
    "    model=INFERENCE_MODEL,\n",
    "    max_results=3\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(f\"ðŸ” Retrieved {result['num_chunks']} relevant chunks\\n\")\n",
    "print(\"ðŸ“„ Context:\")\n",
    "print(\"-\" * 60)\n",
    "print(result['context'][:500] + \"...\" if len(result['context']) > 500 else result['context'])\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"\\nðŸ’¡ Answer:\")\n",
    "print(\"=\" * 60)\n",
    "print(result['answer'])\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Customer Query\n",
    "\n",
    "Test with a question about top customers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸš€ Example 2: Customer Query\n",
      "============================================================\n",
      "\n",
      "â“ Query: Who are the top customers of Parasol Cloud Corp?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://llama-stack-lls-demo.apps.dev.rhoai.rh-aiservices-bu.com/v1/openai/v1/vector_stores/vs_8474850c-2bb6-476e-9ac6-6c5c5e999e0f/search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://llama-stack-lls-demo.apps.dev.rhoai.rh-aiservices-bu.com/v1/openai/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Retrieved 3 relevant chunks\n",
      "\n",
      "ðŸ“„ Context:\n",
      "------------------------------------------------------------\n",
      "Parasol Cloud Corp â€“ FY2025 Earnings Report\n",
      "Executive Summary\n",
      "Parasol Cloud Corp delivered strong performance in FY2025, achieving record revenue\n",
      "growth driven by subscription renewals and strategic upsells. The company maintained\n",
      "a healthy opportunity pipeline, with active opportunities contributing to a projected\n",
      "18% YoY growth. Customer engagement remains strong, with significant\n",
      "improvements in SLA compliance despite an increase in high-severity support cases.\n",
      "\n",
      "Financial Highlights\n",
      "Total rec...\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸ’¡ Answer:\n",
      "============================================================\n",
      "The top customers of Parasol Cloud Corp, based on the provided context, are:\n",
      "\n",
      "1. Acme Corp - representing 65% of total billings, with strong adoption of new cloud packages.\n",
      "2. Globex Inc - representing 65% of total billings, continuing to rely heavily on enterprise licensing.\n",
      "\n",
      "These two companies are the largest contributors to revenue for Parasol Cloud Corp.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸš€ Example 2: Customer Query\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "query = \"Who are the top customers of Parasol Cloud Corp?\"\n",
    "print(f\"â“ Query: {query}\\n\")\n",
    "\n",
    "# Perform agentic RAG\n",
    "result = agentic_rag(\n",
    "    client=client,\n",
    "    vector_store_id=vector_store.id,\n",
    "    query=query,\n",
    "    model=INFERENCE_MODEL,\n",
    "    max_results=3\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(f\"ðŸ” Retrieved {result['num_chunks']} relevant chunks\\n\")\n",
    "print(\"ðŸ“„ Context:\")\n",
    "print(\"-\" * 60)\n",
    "print(result['context'][:500] + \"...\" if len(result['context']) > 500 else result['context'])\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"\\nðŸ’¡ Answer:\")\n",
    "print(\"=\" * 60)\n",
    "print(result['answer'])\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Operational Metrics Query\n",
    "\n",
    "Test with a question about operational performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸš€ Example 3: Operational Metrics Query\n",
      "============================================================\n",
      "\n",
      "â“ Query: What is the SLA compliance rate for Parasol Cloud Corp?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://llama-stack-lls-demo.apps.dev.rhoai.rh-aiservices-bu.com/v1/openai/v1/vector_stores/vs_8474850c-2bb6-476e-9ac6-6c5c5e999e0f/search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://llama-stack-lls-demo.apps.dev.rhoai.rh-aiservices-bu.com/v1/openai/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Retrieved 3 relevant chunks\n",
      "\n",
      "ðŸ“„ Context:\n",
      "------------------------------------------------------------\n",
      " of early, mid, and\n",
      "late-stage deals. The closed opportunities show a natural decline due to the phase-out\n",
      "of legacy contracts, aligning with corporate strategy.\n",
      "\n",
      "Support Case Analysis\n",
      "A total of 6 support cases were logged during this reporting period. 50% of cases were\n",
      "classified as high or critical severity, driven by API reliability issues and dashboard\n",
      "performance incidents. Median resolution time improved by 12% compared to FY2024,\n",
      "highlighting operational efficiencies in the support organ...\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸ’¡ Answer:\n",
      "============================================================\n",
      "The SLA compliance rate for Parasol Cloud Corp is 98.5%.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸš€ Example 3: Operational Metrics Query\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "query = \"What is the SLA compliance rate for Parasol Cloud Corp?\"\n",
    "print(f\"â“ Query: {query}\\n\")\n",
    "\n",
    "# Perform agentic RAG\n",
    "result = agentic_rag(\n",
    "    client=client,\n",
    "    vector_store_id=vector_store.id,\n",
    "    query=query,\n",
    "    model=INFERENCE_MODEL,\n",
    "    max_results=3\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(f\"ðŸ” Retrieved {result['num_chunks']} relevant chunks\\n\")\n",
    "print(\"ðŸ“„ Context:\")\n",
    "print(\"-\" * 60)\n",
    "print(result['context'][:500] + \"...\" if len(result['context']) > 500 else result['context'])\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"\\nðŸ’¡ Answer:\")\n",
    "print(\"=\" * 60)\n",
    "print(result['answer'])\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "We have successfully built an Agentic RAG system using **pure Llama Stack primitives and Responses API** with a real-world earnings report PDF!\n",
    "\n",
    "### What We Demonstrated:\n",
    "\n",
    "âœ… **PDF Download** - Fetched PDF document from URL  \n",
    "âœ… **Vector Store Creation** - Created a vector database with embedded PDF content  \n",
    "âœ… **Document Upload** - Uploaded PDF using the Files API  \n",
    "âœ… **Semantic Search** - Performed vector similarity search on financial data  \n",
    "âœ… **Context Extraction** - Retrieved relevant document chunks from earnings report  \n",
    "âœ… **Response Generation** - Used Responses API for question answering  \n",
    "\n",
    "### Key Llama Stack Primitives Used:\n",
    "\n",
    "| API | Purpose | Method |\n",
    "|-----|---------|--------|\n",
    "| **Files API** | Upload PDF documents | `client.files.create()` |\n",
    "| **Vector Stores API** | Create vector database | `client.vector_stores.create()` |\n",
    "| **Search API** | Semantic search | `client.vector_stores.search()` |\n",
    "| **Responses API** | Generate answers | `client.responses.create()` |\n",
    "\n",
    "### Agentic RAG Workflow:\n",
    "\n",
    "```\n",
    "1. Download PDF from URL\n",
    "   â†“\n",
    "2. Upload to Llama Stack Files API\n",
    "   â†“\n",
    "3. Create vector store with PDF embeddings\n",
    "   â†“\n",
    "4. User asks question\n",
    "   â†“\n",
    "5. vector_stores.search() â†’ Find relevant chunks\n",
    "   â†“\n",
    "6. Extract context from search results\n",
    "   â†“\n",
    "7. Build prompt with context + query\n",
    "   â†“\n",
    "8. responses.create() â†’ Generate answer\n",
    "   â†“\n",
    "9. Return final answer with citations\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "byo-agentic-framework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
