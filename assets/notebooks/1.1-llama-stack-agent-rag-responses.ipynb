{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with Llama Stack Primitives\n",
    "\n",
    "This notebook demonstrates building a **Retrieval-Augmented Generation (RAG)** system using **pure Llama Stack primitives** - no frameworks like LangChain or CrewAI.\n",
    "\n",
    "## Agentic RAG Approach\n",
    "\n",
    "This notebook shows the **Agentic RAG loop**:\n",
    "\n",
    "1. **Query** â†’ User asks a question\n",
    "2. **Search** â†’ Semantic search in vector store\n",
    "3. **Extract** â†’ Get relevant document chunks\n",
    "4. **Generate** â†’ Pass context + query to LLM\n",
    "5. **Answer** â†’ Return generated response\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         User Query                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Vector Search                              â”‚\n",
    "â”‚  (client.vector_stores.search)              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Context Extraction                         â”‚\n",
    "â”‚  (Extract relevant chunks)                  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Llama Stack Responses API                  â”‚\n",
    "â”‚  (client.responses.create)                  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         Final Answer                        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## Configuration\n",
    "\n",
    "This notebook uses environment variables from `.env` file in the project root.\n",
    "Create your own `.env` file based on `.env.example`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Packages\n",
    "\n",
    "Install Llama Stack client (version must match server version):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -q \"llama-stack-client==0.3.0\" \"python-dotenv\" \"requests\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from io import BytesIO\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Llama Stack client\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "# --- Load environment variables ---\n",
    "# Automatically detect the nearest .env (walks up from current directory)\n",
    "env_path = find_dotenv(usecwd=True)\n",
    "if env_path:\n",
    "    load_dotenv(env_path)\n",
    "    print(f\"ðŸ“ Loading environment from: {env_path}\")\n",
    "    print(\"âœ… .env file FOUND and loaded\")\n",
    "else:\n",
    "    default_path = Path.cwd() / \".env\"\n",
    "    print(f\"ðŸ“ No .env found via find_dotenv â€” checked: {default_path}\")\n",
    "    print(\"âš ï¸  .env file NOT FOUND\")\n",
    "\n",
    "# --- Verify Python interpreter / kernel ---\n",
    "print(f\"\\nðŸ Python: {sys.executable}\")\n",
    "\n",
    "# Detect if running inside a virtual environment\n",
    "in_venv = (\n",
    "    hasattr(sys, \"real_prefix\") or\n",
    "    (getattr(sys, \"base_prefix\", sys.prefix) != sys.prefix) or\n",
    "    \"VIRTUAL_ENV\" in os.environ or\n",
    "    \"CONDA_PREFIX\" in os.environ\n",
    ")\n",
    "\n",
    "if in_venv:\n",
    "    print(\"âœ… Using virtual environment - CORRECT!\")\n",
    "else:\n",
    "    print(\"âš ï¸  Using global Python - Consider switching kernel!\")\n",
    "    print(\"   Click 'Select Kernel' â†’ Choose 'Python (byo-agentic-framework)')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Llama Stack Connection\n",
    "\n",
    "Create client using Llama Stack's base URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get configuration from environment variables\n",
    "LLAMA_STACK_BASE_URL = os.getenv(\"LLAMA_STACK_BASE_URL\")\n",
    "INFERENCE_MODEL = os.getenv(\"INFERENCE_MODEL\")\n",
    "LLAMA_STACK_OPENAI_ENDPOINT = os.getenv(\"LLAMA_STACK_OPENAI_ENDPOINT\")\n",
    "\n",
    "print(\"ðŸŒ Llama Stack Configuration:\")\n",
    "print(f\"   Base URL: {LLAMA_STACK_BASE_URL}\")\n",
    "print(f\"   Model: {INFERENCE_MODEL}\")\n",
    "print(f\"   OpenAI Endpoint: {LLAMA_STACK_OPENAI_ENDPOINT}\")\n",
    "\n",
    "# Create client using the base URL\n",
    "client = LlamaStackClient(base_url=LLAMA_STACK_BASE_URL)\n",
    "print(\"\\nâœ… Llama Stack client created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store Setup\n",
    "\n",
    "Create a vector store with Parasol Cloud Corp earnings report.\n",
    "\n",
    "### Step 1: Download PDF Document\n",
    "\n",
    "We'll download the FY2025 earnings report from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“š Downloading Parasol Cloud Corp earnings report...\\n\")\n",
    "\n",
    "# PDF document URL\n",
    "pdf_url = \"https://raw.githubusercontent.com/rh-ai-quickstart/byo-agentic-framework/dev/assets/images/Parasol_Cloud_Corp_Earnings_Report.pdf\"\n",
    "\n",
    "# Download the PDF\n",
    "print(f\"ðŸ“¥ Downloading from: {pdf_url}\")\n",
    "response = requests.get(pdf_url)\n",
    "response.raise_for_status()  # Raise error if download fails\n",
    "\n",
    "pdf_content = response.content\n",
    "print(f\"âœ… Downloaded {len(pdf_content)} bytes\")\n",
    "print(f\"   Content type: application/pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Upload PDF to Llama Stack\n",
    "\n",
    "Upload the earnings report PDF using the Files API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“¤ Uploading PDF to Llama Stack...\\n\")\n",
    "\n",
    "# Create in-memory file from PDF content\n",
    "with BytesIO(pdf_content) as pdf_buffer:\n",
    "    # Set filename for the PDF\n",
    "    pdf_buffer.name = \"Parasol_Cloud_Corp_Earnings_Report.pdf\"\n",
    "    \n",
    "    # Upload to Llama Stack\n",
    "    file_response = client.files.create(\n",
    "        file=pdf_buffer,\n",
    "        purpose=\"assistants\"\n",
    "    )\n",
    "    \n",
    "    file_id = file_response.id\n",
    "    print(f\"âœ… Uploaded: Parasol Cloud Corp FY2025 Earnings Report\")\n",
    "    print(f\"   File ID: {file_id}\")\n",
    "    print(f\"   Filename: Parasol_Cloud_Corp_Earnings_Report.pdf\")\n",
    "\n",
    "file_ids = [file_id]\n",
    "print(f\"\\nâœ… File uploaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create Vector Store\n",
    "\n",
    "Create a vector store with the uploaded files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ”§ Creating vector store...\\n\")\n",
    "\n",
    "vector_store = client.vector_stores.create(\n",
    "    name=\"parasol_earnings_fy2025\",\n",
    "    file_ids=file_ids,\n",
    "    #embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    #embedding_dimension=384,\n",
    "    #provider_id=\"milvus\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Vector store created successfully!\")\n",
    "print(f\"   ID: {vector_store.id}\")\n",
    "print(f\"   Name: {vector_store.name}\")\n",
    "print(f\"   Embedding Model: sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(f\"   Dimensions: 384\")\n",
    "print(f\"   Provider: Milvus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Direct Vector Search\n",
    "\n",
    "Test semantic search by querying the vector store directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ” Testing vector search...\\n\")\n",
    "\n",
    "test_query = \"What is the total revenue for FY2025?\"\n",
    "print(f\"Query: {test_query}\\n\")\n",
    "\n",
    "# Perform semantic search\n",
    "search_response = client.vector_stores.search(\n",
    "    vector_store_id=vector_store.id,\n",
    "    query=test_query,\n",
    "    max_num_results=3\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"ðŸ“Š Search Results:\\n\")\n",
    "for i, result in enumerate(search_response.data, 1):\n",
    "    content = result.content[0].text\n",
    "    filename = result.filename if hasattr(result, 'filename') else 'Unknown'\n",
    "    print(f\"   [{i}] {filename}\")\n",
    "    print(f\"       Content: {content[:200]}...\")  # Show first 200 chars\n",
    "    print()\n",
    "\n",
    "print(\"âœ… Vector search working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agentic RAG Function\n",
    "\n",
    "Define a function that implements the Agentic RAG loop:\n",
    "\n",
    "1. Search the vector store for relevant chunks\n",
    "2. Extract context from search results\n",
    "3. Build a prompt with context + query\n",
    "4. Call Responses API to generate answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agentic_rag(client, vector_store_id, query, model, max_results=3):\n",
    "    \"\"\"\n",
    "    Perform Agentic RAG: search â†’ extract context â†’ generate answer\n",
    "    \n",
    "    Args:\n",
    "        client: LlamaStackClient instance\n",
    "        vector_store_id: ID of the vector store to search\n",
    "        query: User's question\n",
    "        model: Model ID for inference\n",
    "        max_results: Number of chunks to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        dict with query, context, answer, and metadata\n",
    "    \"\"\"\n",
    "    # 1. Search vector store for relevant chunks\n",
    "    search_results = client.vector_stores.search(\n",
    "        vector_store_id=vector_store_id,\n",
    "        query=query,\n",
    "        max_num_results=max_results\n",
    "    )\n",
    "    \n",
    "    # 2. Extract context from search results using traditional loop\n",
    "    context_chunks = []\n",
    "    for result in search_results.data:\n",
    "        text = result.content[0].text\n",
    "        context_chunks.append(text)\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_chunks)\n",
    "    \n",
    "    # 3. Build prompt with context\n",
    "    prompt = f\"\"\"Use the following context to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer based only on the context provided above.\"\"\"\n",
    "    \n",
    "    # 4. Call Responses API to generate answer\n",
    "    response = client.responses.create(\n",
    "        model=model,\n",
    "        input=prompt\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"context\": context,\n",
    "        \"answer\": response.output_text,\n",
    "        \"num_chunks\": len(context_chunks)\n",
    "    }\n",
    "\n",
    "print(\"âœ… Agentic RAG function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Revenue Query\n",
    "\n",
    "Test the RAG system with a financial metrics question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸš€ Example 1: Revenue Query\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "query = \"What is the total revenue of Parasol Cloud Corp in FY2025?\"\n",
    "print(f\"â“ Query: {query}\\n\")\n",
    "\n",
    "# Perform agentic RAG\n",
    "result = agentic_rag(\n",
    "    client=client,\n",
    "    vector_store_id=vector_store.id,\n",
    "    query=query,\n",
    "    model=INFERENCE_MODEL,\n",
    "    max_results=3\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(f\"ðŸ” Retrieved {result['num_chunks']} relevant chunks\\n\")\n",
    "print(\"ðŸ“„ Context:\")\n",
    "print(\"-\" * 60)\n",
    "print(result['context'][:500] + \"...\" if len(result['context']) > 500 else result['context'])\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"\\nðŸ’¡ Answer:\")\n",
    "print(\"=\" * 60)\n",
    "print(result['answer'])\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Customer Query\n",
    "\n",
    "Test with a question about top customers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸš€ Example 2: Customer Query\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "query = \"Who are the top customers of Parasol Cloud Corp?\"\n",
    "print(f\"â“ Query: {query}\\n\")\n",
    "\n",
    "# Perform agentic RAG\n",
    "result = agentic_rag(\n",
    "    client=client,\n",
    "    vector_store_id=vector_store.id,\n",
    "    query=query,\n",
    "    model=INFERENCE_MODEL,\n",
    "    max_results=3\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(f\"ðŸ” Retrieved {result['num_chunks']} relevant chunks\\n\")\n",
    "print(\"ðŸ“„ Context:\")\n",
    "print(\"-\" * 60)\n",
    "print(result['context'][:500] + \"...\" if len(result['context']) > 500 else result['context'])\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"\\nðŸ’¡ Answer:\")\n",
    "print(\"=\" * 60)\n",
    "print(result['answer'])\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Operational Metrics Query\n",
    "\n",
    "Test with a question about operational performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸš€ Example 3: Operational Metrics Query\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "query = \"What is the SLA compliance rate for Parasol Cloud Corp?\"\n",
    "print(f\"â“ Query: {query}\\n\")\n",
    "\n",
    "# Perform agentic RAG\n",
    "result = agentic_rag(\n",
    "    client=client,\n",
    "    vector_store_id=vector_store.id,\n",
    "    query=query,\n",
    "    model=INFERENCE_MODEL,\n",
    "    max_results=3\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(f\"ðŸ” Retrieved {result['num_chunks']} relevant chunks\\n\")\n",
    "print(\"ðŸ“„ Context:\")\n",
    "print(\"-\" * 60)\n",
    "print(result['context'][:500] + \"...\" if len(result['context']) > 500 else result['context'])\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"\\nðŸ’¡ Answer:\")\n",
    "print(\"=\" * 60)\n",
    "print(result['answer'])\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "We have successfully built an Agentic RAG system using **pure Llama Stack primitives and Responses API** with a real-world earnings report PDF!\n",
    "\n",
    "### What We Demonstrated:\n",
    "\n",
    "âœ… **PDF Download** - Fetched PDF document from URL  \n",
    "âœ… **Vector Store Creation** - Created a vector database with embedded PDF content  \n",
    "âœ… **Document Upload** - Uploaded PDF using the Files API  \n",
    "âœ… **Semantic Search** - Performed vector similarity search on financial data  \n",
    "âœ… **Context Extraction** - Retrieved relevant document chunks from earnings report  \n",
    "âœ… **Response Generation** - Used Responses API for question answering  \n",
    "\n",
    "### Key Llama Stack Primitives Used:\n",
    "\n",
    "| API | Purpose | Method |\n",
    "|-----|---------|--------|\n",
    "| **Files API** | Upload PDF documents | `client.files.create()` |\n",
    "| **Vector Stores API** | Create vector database | `client.vector_stores.create()` |\n",
    "| **Search API** | Semantic search | `client.vector_stores.search()` |\n",
    "| **Responses API** | Generate answers | `client.responses.create()` |\n",
    "\n",
    "### Agentic RAG Workflow:\n",
    "\n",
    "```\n",
    "1. Download PDF from URL\n",
    "   â†“\n",
    "2. Upload to Llama Stack Files API\n",
    "   â†“\n",
    "3. Create vector store with PDF embeddings\n",
    "   â†“\n",
    "4. User asks question\n",
    "   â†“\n",
    "5. vector_stores.search() â†’ Find relevant chunks\n",
    "   â†“\n",
    "6. Extract context from search results\n",
    "   â†“\n",
    "7. Build prompt with context + query\n",
    "   â†“\n",
    "8. responses.create() â†’ Generate answer\n",
    "   â†“\n",
    "9. Return final answer with citations\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "byo-agentic-framework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
