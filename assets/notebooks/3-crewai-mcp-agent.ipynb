{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrewAI Agents with Llama Stack and RAG\n",
    "\n",
    "This notebook demonstrates **CrewAI** integration with **Llama Stack** to build a Retrieval-Augmented Generation (RAG) system.\n",
    "\n",
    "## Overview\n",
    "\n",
    "- **Llama Stack**: Provides the infrastructure for running LLMs and vector store\n",
    "- **CrewAI**: Offers a framework for orchestrating agents and tasks\n",
    "- **Integration**: Leverages Llama Stack's OpenAI-compatible API with CrewAI\n",
    "\n",
    "## Configuration\n",
    "\n",
    "This notebook uses environment variables from `.env` file in the project root.\n",
    "Create your own `.env` file based on `.env.example`.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ         CrewAI Agent                        ‚îÇ\n",
    "‚îÇ         (Agent + Task + Crew)               ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚îÇ                    \n",
    "         ‚îÇ Model Calls        \n",
    "         ‚ñº                    \n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Llama Stack    ‚îÇ   ‚îÇ  RAG Tool            ‚îÇ\n",
    "‚îÇ  (OpenAI API)   ‚îÇ   ‚îÇ  (CrewAI)            ‚îÇ\n",
    "‚îÇ  - vLLM Engine  ‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ  - Vector Search     ‚îÇ\n",
    "‚îÇ  - Inference    ‚îÇ   ‚îÇ  - Document Retrieval‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                             ‚îÇ\n",
    "                      Tool Execution\n",
    "                             ‚ñº\n",
    "                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                   ‚îÇ Llama Stack      ‚îÇ\n",
    "                   ‚îÇ Vector Store API ‚îÇ\n",
    "                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Packages\n",
    "\n",
    "Install CrewAI and dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q --upgrade \\\n",
    "  \"crewai>=0.76.0\" \\\n",
    "  \"crewai-tools>=0.33.0\" \\\n",
    "  \"chromadb>=0.5.5\" \\\n",
    "  \"pydantic<3\" \\\n",
    "  \"pysqlite3-binary>=0.5.3.post3\" \\\n",
    "  \"ipywidgets==8.1.8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "\n",
    "# Force Python to use the modern SQLite provided by pysqlite3-binary\n",
    "pysqlite3 = importlib.import_module(\"pysqlite3\")\n",
    "sys.modules[\"sqlite3\"] = pysqlite3\n",
    "sys.modules[\"sqlite\"] = pysqlite3  # some libs import 'sqlite' directly\n",
    "\n",
    "# (Optional) sanity check\n",
    "import sqlite3\n",
    "print(\"SQLite version:\", sqlite3.sqlite_version)  # should be >= 3.35.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Any, List, Optional, Type\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from io import BytesIO\n",
    "\n",
    "# Llama Stack client\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "# CrewAI imports\n",
    "from crewai import Agent, Crew, Task\n",
    "from crewai.llm import LLM\n",
    "from crewai.tools import BaseTool\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# --- Load environment variables ---\n",
    "# Automatically detect the nearest .env (walks up from current directory)\n",
    "env_path = find_dotenv(usecwd=True)\n",
    "if env_path:\n",
    "    load_dotenv(env_path)\n",
    "    print(f\"üìÅ Loading environment from: {env_path}\")\n",
    "    print(\"‚úÖ .env file FOUND and loaded\")\n",
    "else:\n",
    "    default_path = Path.cwd() / \".env\"\n",
    "    print(f\"üìÅ No .env found via find_dotenv ‚Äî checked: {default_path}\")\n",
    "    print(\"‚ö†Ô∏è  .env file NOT FOUND\")\n",
    "\n",
    "# --- Verify Python interpreter / kernel ---\n",
    "print(f\"\\nüêç Python: {sys.executable}\")\n",
    "\n",
    "# Detect if running inside a virtual environment\n",
    "in_venv = (\n",
    "    hasattr(sys, \"real_prefix\") or\n",
    "    (getattr(sys, \"base_prefix\", sys.prefix) != sys.prefix) or\n",
    "    \"VIRTUAL_ENV\" in os.environ or\n",
    "    \"CONDA_PREFIX\" in os.environ\n",
    ")\n",
    "\n",
    "if in_venv:\n",
    "    print(\"‚úÖ Using virtual environment - CORRECT!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Using global Python - Consider switching kernel!\")\n",
    "    print(\"   Click 'Select Kernel' ‚Üí Choose 'Python (byo-agentic-framework)')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Llama Stack Connection\n",
    "\n",
    "Connect to Llama Stack's OpenAI-compatible endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Llama Stack Configuration ===\n",
    "# Load from environment variables\n",
    "LLAMA_STACK_BASE_URL = os.getenv(\"LLAMA_STACK_BASE_URL\")\n",
    "LLAMA_STACK_OPENAI_ENDPOINT = os.getenv(\"LLAMA_STACK_OPENAI_ENDPOINT\")\n",
    "INFERENCE_MODEL = os.getenv(\"INFERENCE_MODEL\")\n",
    "API_KEY = os.getenv(\"API_KEY\", \"fake\")\n",
    "\n",
    "print(\"üåê Llama Stack Configuration:\")\n",
    "print(f\"   Base URL: {LLAMA_STACK_BASE_URL}\")\n",
    "print(f\"   OpenAI Endpoint: {LLAMA_STACK_OPENAI_ENDPOINT}\")\n",
    "print(f\"   Model: {INFERENCE_MODEL}\")\n",
    "\n",
    "# Initialize Llama Stack Client\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "client = LlamaStackClient(\n",
    "    base_url=LLAMA_STACK_BASE_URL,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LlamaStack client initialized\")\n",
    "\n",
    "# Create CrewAI LLM instance pointing to Llama Stack\n",
    "from crewai.llm import LLM\n",
    "\n",
    "llm = LLM(\n",
    "    model=f\"openai/{INFERENCE_MODEL}\",  # OpenAI-compatible model format\n",
    "    base_url=LLAMA_STACK_OPENAI_ENDPOINT,\n",
    "    api_key=API_KEY,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "print(\"\\\\nüß™ Testing connectivity...\")\n",
    "try:\n",
    "    response = llm.call([\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Say 'Connection successful' if you can read this.\"}\n",
    "    ])\n",
    "    print(f\"üì• LLM Response: {response}\")\n",
    "    print(\"‚úÖ Llama Stack connection successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection failed: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store Setup\n",
    "\n",
    "### Create a Vector Store with Sample Documents\n",
    "\n",
    "Create a vector store using Llama Stack's OpenAI-compatible vector stores API:\n",
    "\n",
    "- **Vector Store**: OpenAI-compatible vector store for document storage\n",
    "- **File Upload**: Automatic chunking and embedding of uploaded files  \n",
    "- **Embedding Model**: Sentence Transformers model for text embeddings\n",
    "- **Dimensions**: 384-dimensional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìö Setting up vector store with Acme documentation...\\n\")\n",
    "\n",
    "# Sample documents for RAG\n",
    "docs = [\n",
    "    (\"Acme ships globally in 3-5 business days.\", {\"title\": \"Shipping Policy\"}),\n",
    "    (\"Returns are accepted within 30 days of purchase.\", {\"title\": \"Returns Policy\"}),\n",
    "    (\"Support is available 24/7 via chat and email.\", {\"title\": \"Support\"}),\n",
    "]\n",
    "\n",
    "# Upload files using Llama Stack client\n",
    "file_ids = []\n",
    "print(\"üì§ Uploading documents to Llama Stack...\")\n",
    "for content, metadata in docs:\n",
    "    # Llama Stack's file API expects a file object (not string), so we create an in-memory file.\n",
    "    with BytesIO(content.encode()) as file_buffer:\n",
    "        # Set a filename for the in-memory file\n",
    "        file_buffer.name = f\"{metadata['title'].replace(' ', '_').lower()}.txt\"\n",
    "        # Upload to Llama Stack\n",
    "        create_file_response = client.files.create(\n",
    "            file=file_buffer,\n",
    "            purpose=\"assistants\"\n",
    "        )\n",
    "        print(f\"   ‚úÖ Uploaded: {metadata['title']} (ID: {create_file_response.id})\")\n",
    "        # Store file ID for vector store creation to later use\n",
    "        file_ids.append(create_file_response.id)\n",
    "\n",
    "# Create vector store with uploaded files\n",
    "print(\"\\nüîß Creating vector store...\")\n",
    "vector_store = client.vector_stores.create(\n",
    "    name=\"acme_docs\",\n",
    "    file_ids=file_ids,  # Use uploaded file IDs\n",
    "    #embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    #embedding_dimension=384,\n",
    "    #provider_id=\"milvus\" # Use Milvus as the vector store provider\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Vector store created successfully!\")\n",
    "print(f\"   ID: {vector_store.id}\")\n",
    "print(f\"   Name: {vector_store.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Knowledge Base Search\n",
    "\n",
    "Test our simple keyword-based search functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Testing vector search...\\n\")\n",
    "\n",
    "# Perform semantic search on the vector store\n",
    "search_response = client.vector_stores.search(\n",
    "    vector_store_id=vector_store.id,\n",
    "    query=\"How long does shipping take?\",\n",
    "    max_num_results=2\n",
    ")\n",
    "\n",
    "# Display the most relevant documents found\n",
    "print(\"üìä Search Results:\")\n",
    "for result in search_response.data:\n",
    "    content = result.content[0].text\n",
    "    print(f\"   - {content}\")\n",
    "\n",
    "print(\"\\n‚úÖ Vector search working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CrewAI Custom RAG Tool\n",
    "\n",
    "Define a custom CrewAI tool to query the knowledge base:\n",
    "\n",
    "- **Input Schema**: Defines the user query and optional parameters like `top_k`\n",
    "- **Tool Logic**: Performs keyword-based search and returns formatted results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 1. Input schema ----------\n",
    "class VectorStoreRAGToolInput(BaseModel):\n",
    "    \"\"\"Input schema for LlamaStackVectorStoreRAGTool.\"\"\"\n",
    "    query: str = Field(..., description=\"The user query for RAG search\")\n",
    "    vector_store_id: str = Field(...,\n",
    "        description=\"ID of the vector store to search inside the Llama-Stack server\",\n",
    "    )\n",
    "    top_k: Optional[int] = Field(\n",
    "        default=5,\n",
    "        description=\"How many documents to return\",\n",
    "    )\n",
    "    score_threshold: Optional[float] = Field(\n",
    "        default=None,\n",
    "        description=\"Optional similarity score cut-off (0-1).\",\n",
    "    )\n",
    "\n",
    "# ---------- 2. The RAG tool ----------\n",
    "class LlamaStackVectorStoreRAGTool(BaseTool):\n",
    "    name: str = \"Llama Stack Vector Store RAG tool\"\n",
    "    description: str = (\n",
    "        \"This tool calls a Llama-Stack endpoint for retrieval-augmented generation using a vector store. \"\n",
    "        \"It takes a natural-language query and returns the most relevant documents.\"\n",
    "    )\n",
    "    args_schema: Type[BaseModel] = VectorStoreRAGToolInput\n",
    "    client: Any\n",
    "    vector_store_id: str = \"\"\n",
    "    top_k: int = 5\n",
    "\n",
    "    def _run(self, **kwargs: Any) -> str:\n",
    "        # Helper function to extract actual values from CrewAI's Field metadata dicts\n",
    "        def extract_value(val, default=None):\n",
    "            \"\"\"Extract actual value from CrewAI's Field metadata dictionary.\"\"\"\n",
    "            if isinstance(val, dict) and 'description' in val:\n",
    "                # CrewAI passes Field metadata, extract the actual value\n",
    "                # The dict looks like: {'description': 'actual_value', 'type': 'str'}\n",
    "                return val.get('description', default)\n",
    "            return val if val is not None else default\n",
    "        \n",
    "        # 1. Resolve parameters and extract actual values from CrewAI metadata\n",
    "        query_raw = kwargs.get(\"query\")\n",
    "        vector_store_id_raw = kwargs.get(\"vector_store_id\", self.vector_store_id)\n",
    "        top_k_raw = kwargs.get(\"top_k\", self.top_k)\n",
    "        \n",
    "        # Extract actual values\n",
    "        query: str = extract_value(query_raw)\n",
    "        vector_store_id: str = extract_value(vector_store_id_raw, self.vector_store_id)\n",
    "        top_k: int = extract_value(top_k_raw, self.top_k)\n",
    "        \n",
    "        # Convert top_k to int if it's still a string\n",
    "        if isinstance(top_k, str):\n",
    "            top_k = int(top_k)\n",
    "        \n",
    "        if not vector_store_id or vector_store_id == \"\":\n",
    "            print('vector_store_id is empty, please specify which vector_store to search')\n",
    "            return \"No documents found.\"\n",
    "            \n",
    "        # 2. Issue request to Llama-Stack\n",
    "        response = self.client.vector_stores.search(\n",
    "            vector_store_id=vector_store_id,\n",
    "            query=query,\n",
    "            max_num_results=top_k,\n",
    "        )\n",
    "\n",
    "        # 3. Massage results into a single human-readable string\n",
    "        if not response or not response.data:\n",
    "            return \"No documents found.\"\n",
    "\n",
    "        docs: List[str] = []\n",
    "        for result in response.data:\n",
    "            content = result.content[0].text if result.content else \"No content\"\n",
    "            filename = result.filename if result.filename else {}\n",
    "            docs.append(f\"filename: {filename}, content: {content}\")\n",
    "        return \"\\\\n\".join(docs)\n",
    "\n",
    "# Create tool instance with Llama Stack client\n",
    "rag_tool = LlamaStackVectorStoreRAGTool(\n",
    "    client=client,\n",
    "    vector_store_id=\"\",  # Will be provided at runtime\n",
    "    top_k=3\n",
    ")\n",
    "\n",
    "print(\"\\\\n‚úÖ RAG Tool configured successfully!\")\n",
    "print(f\"   Tool Name: {rag_tool.name}\")\n",
    "print(f\"   Uses: Llama Stack Vector Store API\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the RAG Agent\n",
    "\n",
    "### Create a Complete RAG Pipeline\n",
    "\n",
    "Construct a CrewAI pipeline that orchestrates the RAG process:\n",
    "\n",
    "1. **Agent Definition**: CrewAI agent with RAG capabilities\n",
    "2. **Task Definition**: Task for answering questions using retrieved context\n",
    "3. **Crew Definition**: Complete RAG pipeline\n",
    "\n",
    "**CrewAI workflow**:\n",
    "`User Query ‚Üí CrewAI Task ‚Üí Agent invokes RAG Tool ‚Üí Llama Stack Vector Search ‚Üí Retrieved Context ‚Üí LLM Generation ‚Üí Final Response`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ü§ñ Creating CrewAI RAG agent...\\\\n\")\n",
    "\n",
    "# Define the RAG agent\n",
    "agent = Agent(\n",
    "    role=\"RAG assistant\",\n",
    "    goal=\"Answer user's questions with provided context\",\n",
    "    backstory=\"You are an experienced search assistant specializing in finding relevant information from documentation and vector_db to answer user questions accurately.\",\n",
    "    allow_delegation=False,\n",
    "    llm=llm,\n",
    "    tools=[rag_tool],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Define the task\n",
    "task = Task(\n",
    "    description=\"Answer the following question: {query}. Use the RAG tool to search the provided vector_store_id {vector_store_id} if needed.\",\n",
    "    expected_output=\"A clear and accurate answer to the question based on the retrieved context\",\n",
    "    agent=agent,\n",
    ")\n",
    "\n",
    "# Create the crew\n",
    "crew = Crew(\n",
    "    agents=[agent],\n",
    "    tasks=[task],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RAG Pipeline created successfully!\")\n",
    "print(\"\\\\nüìä Configuration:\")\n",
    "print(f\"   Agent Role: {agent.role}\")\n",
    "print(f\"   Model: {INFERENCE_MODEL}\")\n",
    "print(f\"   Tools: {len(agent.tools)} tool(s)\")\n",
    "print(f\"   Framework: CrewAI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the RAG System\n",
    "\n",
    "### Example 1: Shipping Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"üöÄ Testing CrewAI RAG Agent\")\n",
    "print(\"=\"*60 + \"\\\\n\")\n",
    "\n",
    "query = \"How long does shipping take?\"\n",
    "print(f\"‚ùì {query}\\\\n\")\n",
    "\n",
    "response = crew.kickoff(inputs={\"query\": query, \"vector_store_id\": vector_store.id})\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(f\"üí° {response}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Returns Policy Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"üöÄ Testing CrewAI RAG Agent\")\n",
    "print(\"=\"*60 + \"\\\\n\")\n",
    "\n",
    "query = \"What is the return policy?\"\n",
    "print(f\"‚ùì {query}\\\\n\")\n",
    "\n",
    "response = crew.kickoff(inputs={\"query\": query, \"vector_store_id\": vector_store.id})\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(f\"üí° {response}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Support Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How can I contact support?\"\n",
    "print(f\"\\\\n‚ùì {query}\\\\n\")\n",
    "\n",
    "response = crew.kickoff(inputs={\"query\": query, \"vector_store_id\": vector_store.id})\n",
    "\n",
    "print(f\"\\\\nüí° {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "We have successfully built a RAG system that combines:\n",
    "\n",
    "- **Llama Stack** for LLM inference via OpenAI-compatible API\n",
    "- **CrewAI** for agent orchestration (agents, tasks, and tools)\n",
    "- **Custom RAG Tool** for document retrieval from knowledge base"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
