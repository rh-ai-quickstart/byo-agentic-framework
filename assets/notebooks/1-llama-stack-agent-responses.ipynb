{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21547149",
   "metadata": {},
   "source": [
    "# Building Agents with Llama Stack Agentic Capabilities\n",
    "\n",
    "This notebook demonstrates building **agentic systems** using Llama Stack's native capabilities for tool calling with **MCP (Model Context Protocol)** integration.\n",
    "\n",
    "For more information check [the Llama Stack Responses API docs](https://llamastack.github.io/docs/building_applications/responses_vs_agents#lls-agents-api) and the [OpenAI Responses API](https://platform.openai.com/docs/api-reference/responses).\n",
    "\n",
    "## Configuration\n",
    "\n",
    "This notebook uses environment variables from `.env` file in the project root.\n",
    "Create your own `.env` file based on `.env.example`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install-header",
   "metadata": {},
   "source": [
    "## Install Required Packages\n",
    "\n",
    "Install Llama Stack client (version must match server version):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f525a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q \"llama-stack-client==0.2.22\" \"python-dotenv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "import-header",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jsl1ilhed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# --- Load environment variables ---\n",
    "# Automatically detect the nearest .env (walks up from current directory)\n",
    "env_path = find_dotenv(usecwd=True)\n",
    "if env_path:\n",
    "    load_dotenv(env_path)\n",
    "    print(f\"üìÅ Loading environment from: {env_path}\")\n",
    "    print(\"‚úÖ .env file FOUND and loaded\")\n",
    "else:\n",
    "    default_path = Path.cwd() / \".env\"\n",
    "    print(f\"üìÅ No .env found via find_dotenv ‚Äî checked: {default_path}\")\n",
    "    print(\"‚ö†Ô∏è  .env file NOT FOUND\")\n",
    "\n",
    "# --- Verify Python interpreter / kernel ---\n",
    "print(f\"\\nüêç Python: {sys.executable}\")\n",
    "\n",
    "# Detect if running inside a virtual environment\n",
    "in_venv = (\n",
    "    hasattr(sys, \"real_prefix\") or\n",
    "    (getattr(sys, \"base_prefix\", sys.prefix) != sys.prefix) or\n",
    "    \"VIRTUAL_ENV\" in os.environ or\n",
    "    \"CONDA_PREFIX\" in os.environ\n",
    ")\n",
    "\n",
    "if in_venv:\n",
    "    print(\"‚úÖ Using virtual environment - CORRECT!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Using global Python - Consider switching kernel!\")\n",
    "    print(\"   Click 'Select Kernel' ‚Üí Choose 'Python (byo-agentic-framework)')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## Configure Llama Stack Connection\n",
    "\n",
    "Create client using Llama Stack's base URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13094af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "# Get configuration from environment variables\n",
    "LLAMA_STACK_BASE_URL = os.getenv(\"LLAMA_STACK_BASE_URL\")\n",
    "LLAMA_STACK_OPENAI_ENDPOINT = os.getenv(\"LLAMA_STACK_OPENAI_ENDPOINT\")\n",
    "\n",
    "print(\"üåê Llama Stack Configuration:\")\n",
    "print(f\"   Base URL: {LLAMA_STACK_BASE_URL}\")\n",
    "print(f\"   OpenAI Endpoint: {LLAMA_STACK_OPENAI_ENDPOINT}\")\n",
    "\n",
    "# Create client using the base URL\n",
    "client = LlamaStackClient(base_url=LLAMA_STACK_BASE_URL)\n",
    "print(\"‚úÖ Llama Stack client created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helper-header",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Utility for pretty-printing response objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48255cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import date\n",
    "\n",
    "def pretty_print(obj) -> None:\n",
    "    \"\"\"\n",
    "    Print object as formatted JSON.\n",
    "    Handles nested objects and lists.\n",
    "    \"\"\"\n",
    "    def recursive_serializer(o):\n",
    "        if hasattr(o, '__dict__'):\n",
    "            return o.__dict__\n",
    "        if isinstance(o, date):\n",
    "            return o.isoformat()\n",
    "        raise TypeError(f\"Object of type {o.__class__.__name__} is not JSON serializable\")\n",
    "\n",
    "    data_to_serialize = obj.__dict__ if hasattr(obj, \"__dict__\") else obj\n",
    "    print(json.dumps(data_to_serialize, indent=2, default=recursive_serializer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example1-header",
   "metadata": {},
   "source": [
    "## Example 1: Agent with Tools Available (No Tool Usage)\n",
    "\n",
    "Test the agent with a basic question. The agent has tools available but determines they're not needed for this query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8f3b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "INFERENCE_MODEL = os.getenv(\"INFERENCE_MODEL\")\n",
    "MCP_WEATHER_SERVER_URL = os.getenv(\"MCP_WEATHER_SERVER_URL\")\n",
    "\n",
    "EXAMPLE_PROMPT = \"What is the longest river in Spain?\"\n",
    "\n",
    "print(f\"ü§ñ Model: {INFERENCE_MODEL}\")\n",
    "print(f\"üí¨ Prompt: {EXAMPLE_PROMPT}\\n\")\n",
    "\n",
    "# Use Llama Stack's Responses API with MCP tools available\n",
    "responses = client.responses.create(\n",
    "    model=INFERENCE_MODEL,\n",
    "    input=EXAMPLE_PROMPT,\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FINAL RESPONSE:\")\n",
    "print(\"=\" * 80)\n",
    "print(responses.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example2-header",
   "metadata": {},
   "source": [
    "## Example 2: Agentic Tool Execution\n",
    "\n",
    "Test the agent with a weather query. The agent will:\n",
    "1. **Discover** available MCP tools\n",
    "2. **Decide** which tool to use (getforecast)\n",
    "3. **Execute** the tool call with appropriate arguments\n",
    "4. **Synthesize** the tool results into a natural language response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75f3d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "INFERENCE_MODEL = os.getenv(\"INFERENCE_MODEL\")\n",
    "MCP_WEATHER_SERVER_URL = os.getenv(\"MCP_WEATHER_SERVER_URL\")\n",
    "\n",
    "EXAMPLE_PROMPT = \"What is the weather in Boston? Use the getforecast tool.\"\n",
    "\n",
    "print(f\"ü§ñ Model: {INFERENCE_MODEL}\")\n",
    "print(f\"üå¶Ô∏è  MCP Server: {MCP_WEATHER_SERVER_URL}\")\n",
    "print(f\"üí¨ Prompt: {EXAMPLE_PROMPT}\\n\")\n",
    "\n",
    "# Use Llama Stack's Responses API\n",
    "agent_responses = client.responses.create(\n",
    "    model=INFERENCE_MODEL,\n",
    "    input=EXAMPLE_PROMPT,\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"mcp\",  # Server-side MCP\n",
    "            \"server_url\": MCP_WEATHER_SERVER_URL,\n",
    "            \"server_label\": \"weather\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Display execution trace\n",
    "print(\"ü§ñ Agent Execution Trace:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i, output in enumerate(agent_responses.output):\n",
    "    print(f\"\\n[Output {i + 1}] Type: {output.type}\")\n",
    "    \n",
    "    if output.type == \"mcp_list_tools\":\n",
    "        print(f\"  Server: {output.server_label}\")\n",
    "        print(f\"  Tools available: {[t.name for t in output.tools]}\")\n",
    "    \n",
    "    elif output.type == \"mcp_call\":\n",
    "        print(f\"  Tool called: {output.name}\")\n",
    "        print(f\"  Arguments: {output.arguments}\")\n",
    "        print(f\"  Result: {output.output[:200]}...\")  # Truncate long output\n",
    "        if output.error:\n",
    "            print(f\"  Error: {output.error}\")\n",
    "    \n",
    "    elif output.type == \"message\":\n",
    "        print(f\"  Role: {output.role}\")\n",
    "        if hasattr(output.content[0], 'text'):\n",
    "            print(f\"  Content: {output.content[0].text}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL RESPONSE:\")\n",
    "print(\"=\" * 80)\n",
    "print(agent_responses.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847005c6",
   "metadata": {},
   "source": [
    "## Example 3: Agentic Tool Execution - Kubernetes MCP Server\n",
    "\n",
    "Test the agent with a OpenShift query. The agent will:\n",
    "1. **Discover** available MCP tools\n",
    "2. **Decide** which tool to use (getforecast)\n",
    "3. **Execute** the tool call with appropriate arguments\n",
    "4. **Synthesize** the tool results into a natural language response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86902c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "INFERENCE_MODEL = os.getenv(\"INFERENCE_MODEL\")\n",
    "MCP_OCP_SERVER_URL = os.getenv(\"MCP_OCP_SERVER_URL\")\n",
    "\n",
    "EXAMPLE_PROMPT = \"Give me a list of the pods in the ai-bu-shared. Use the k8s tool.\"\n",
    "\n",
    "print(f\"ü§ñ Model: {INFERENCE_MODEL}\")\n",
    "print(f\"üå¶Ô∏è  MCP Server: {MCP_OCP_SERVER_URL}\")\n",
    "print(f\"üí¨ Prompt: {EXAMPLE_PROMPT}\\n\")\n",
    "\n",
    "# Use Llama Stack's Responses API\n",
    "agent_responses = client.responses.create(\n",
    "    model=INFERENCE_MODEL,\n",
    "    input=EXAMPLE_PROMPT,\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"mcp\",  # Server-side MCP\n",
    "            \"server_url\": MCP_OCP_SERVER_URL,\n",
    "            \"server_label\": \"k8s\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Display execution trace\n",
    "print(\"ü§ñ Agent Execution Trace:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i, output in enumerate(agent_responses.output):\n",
    "    print(f\"\\n[Output {i + 1}] Type: {output.type}\")\n",
    "    \n",
    "    if output.type == \"mcp_list_tools\":\n",
    "        print(f\"  Server: {output.server_label}\")\n",
    "        print(f\"  Tools available: {[t.name for t in output.tools]}\")\n",
    "    \n",
    "    elif output.type == \"mcp_call\":\n",
    "        print(f\"  Tool called: {output.name}\")\n",
    "        print(f\"  Arguments: {output.arguments}\")\n",
    "        print(f\"  Result: {output.output[:200]}...\")  # Truncate long output\n",
    "        if output.error:\n",
    "            print(f\"  Error: {output.error}\")\n",
    "    \n",
    "    elif output.type == \"message\":\n",
    "        print(f\"  Role: {output.role}\")\n",
    "        if hasattr(output.content[0], 'text'):\n",
    "            print(f\"  Content: {output.content[0].text}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL RESPONSE:\")\n",
    "print(\"=\" * 80)\n",
    "print(agent_responses.output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
